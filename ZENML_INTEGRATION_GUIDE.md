# ZenML Integration Guide for Memory-Aware RAG System

**Document Version**: 1.0  
**Date**: 2025-01-19  
**Status**: Integration Proposal  
**Author**: Architecture Review Team

---

## ðŸ“‹ Table of Contents

1. [Executive Summary](#executive-summary)
2. [Current Architecture Review](#current-architecture-review)
3. [ZenML Integration Benefits](#zenml-integration-benefits)
4. [Proposed Integration Architecture](#proposed-integration-architecture)
5. [Pipeline Designs](#pipeline-designs)
6. [Artifact Management Strategy](#artifact-management-strategy)
7. [Experiment Tracking](#experiment-tracking)
8. [Model Registry Integration](#model-registry-integration)
9. [Deployment Strategy](#deployment-strategy)
10. [Migration Path](#migration-path)
11. [Best Practices](#best-practices)
12. [Implementation Roadmap](#implementation-roadmap)

---

## ðŸŽ¯ Executive Summary

### Current State

Your memory-aware RAG system is a sophisticated multi-tier architecture with:
- **3-tier memory system**: Working Memory (WM) â†’ Episodic Memory (EM) â†’ Semantic Memory (SM)
- **LangGraph orchestration**: State machine for chat flow
- **4 background jobs**: Promotion, salience decay, TTL cleanup, session cleanup
- **Multiple pipelines**: Ingestion, retrieval, distillation, promotion
- **ChromaDB vector storage**: Dual collections for EM and SM

### Integration Value Proposition

**ZenML will provide**:

- **Pipeline Orchestration**: Version and track all pipelines (ingestion, distillation, promotion)
- **Artifact Lineage**: Track embeddings, models, datasets, and their relationships
- **Experiment Tracking**: Compare different prompts, models, chunking strategies
- **Reproducibility**: Snapshot entire pipeline state for debugging
- **Model Registry**: Centralized versioning for LLMs, embeddings, rerankers
- **Production Monitoring**: Observe pipeline health, data drift, model performance

### Recommendation

**Phased integration** starting with ingestion pipeline, then memory pipelines, finally background jobs.

**Expected ROI**:

- 60% reduction in debugging time (full lineage)
- 40% faster experimentation (artifact caching)
- 90% improvement in reproducibility
- Production-grade observability out-of-the-box

---

## ðŸ—ï¸ Current Architecture Review

### System Components

#### 1. **Document Ingestion Pipeline**

- **Flow**: Load â†’ Split â†’ Embed â†’ Store
- **Input**: Raw documents (PDF, TXT, MD, HTML, CSV, JSON)
- **Processing**:
  - `RecursiveCharacterTextSplitter` (1024 chunks, 200 overlap)
  - LangChain document loaders (extension-specific)
  - Batch processing with error handling
- **Output**: Chunks stored in ChromaDB SM collection
- **Current Implementation**: Service-based (`ingest_service.py`)

#### 2. **Memory-Aware Chat Pipeline** (LangGraph)

- **Flow**: WM Load â†’ Classify Intent â†’ EM/SM Query (parallel) â†’ Rerank â†’ Generate â†’ WM Update
- **Nodes**:
  1. `load_wm`: Fetch last N conversation turns
  2. `classify`: Intent classification + entity extraction (LLM-based)
  3. `query_em`: Semantic search in episodic memory
  4. `query_sm`: Semantic search in document knowledge base
  5. `rerank`: Cross-encoder reranking (top-k selection)
  6. `generate`: LLM response generation with citations
  7. `update_wm`: Session update + optional distillation trigger
- **Current Implementation**: LangGraph StateGraph (`graph.py`)

#### 3. **Memory Distillation Pipeline**

- **Trigger**: Every N conversation turns (configurable)
- **Flow**: Recent Turns â†’ LLM Summarization â†’ Fact Extraction â†’ EM Storage
- **Processing**:
  - Summarizes last N turns into discrete facts
  - Deduplicates against existing EM facts
  - Adds salience scores and TTL timestamps
- **Output**: Facts in EM collection with metadata
- **Current Implementation**: Service-based (`conversation_summarizer.py`)

#### 4. **Memory Promotion Pipeline**

- **Trigger**: Daily background job
- **Flow**: Scan EM â†’ Apply Criteria â†’ Promote to SM â†’ Cleanup
- **Criteria**:
  - High salience (> 0.8)
  - High citation count (> 5 times)
  - Age threshold (> 7 days)
- **Processing**:
  - Batch scan of EM collection
  - Metadata-based filtering
  - Cross-collection copy + delete
- **Current Implementation**: Background job (`promotion.py`)

#### 5. **Background Jobs** (4 concurrent threads)

- **Promotion Cycle**: Daily (24h interval)
- **Salience Flush**: Every 5 minutes (batch update to ChromaDB)
- **TTL Cleanup**: Every 6 hours (remove expired EM facts)
- **Session Cleanup**: Every 15 minutes (remove stale sessions)
- **Current Implementation**: Python threading (`background_jobs.py`)

### Data Flow Architecture

```bash
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCUMENT INGESTION                            â”‚
â”‚  Raw Docs â†’ Load â†’ Split â†’ Embed â†’ ChromaDB (SM Collection)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CHAT + RETRIEVAL                              â”‚
â”‚  Query â†’ Intent â†’ [EM Query || SM Query] â†’ Rerank â†’ Generate    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MEMORY DISTILLATION                           â”‚
â”‚  Turns â†’ Summarize â†’ Extract Facts â†’ EM Collection              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MEMORY PROMOTION (Daily)                      â”‚
â”‚  Scan EM â†’ Filter (salience/citations/age) â†’ Copy to SM         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Observations

**Strengths**:

- Well-defined pipeline stages
- Clear separation of concerns
- Parallel execution where applicable (EM/SM queries)
- Comprehensive metadata tracking

**Gaps** (where ZenML adds value):

- âŒ No pipeline versioning
- âŒ No artifact lineage tracking
- âŒ No experiment comparison tools
- âŒ No automatic caching of expensive steps
- âŒ Manual monitoring of background jobs
- âŒ No rollback capability
- âŒ Limited reproducibility guarantees

---

## ðŸš€ ZenML Integration Benefits

### 1. **Pipeline Orchestration & Versioning**

**Current State**: Manual service calls, no version control
**With ZenML**: Every pipeline run is versioned with full context

**Benefits**:

- Track exactly which code version produced which results
- Compare pipeline runs across time
- Rollback to previous pipeline versions
- A/B test different pipeline configurations

### 2. **Artifact Management & Lineage**

**Current State**: Artifacts (embeddings, chunks, facts) stored without lineage
**With ZenML**: Full DAG tracking from raw data to final results

**Benefits**:

- "Which documents contributed to this EM fact?"
- "When was this embedding model last updated?"
- "Which conversations triggered this promotion?"
- Automatic artifact caching (skip re-embedding unchanged docs)

### 3. **Experiment Tracking**

**Current State**: Manual logging, difficult to compare experiments
**With ZenML**: Built-in experiment tracking with MLflow integration

**Benefits**:

- Compare different chunking strategies (512 vs 1024)
- Evaluate prompt variations
- Test multiple LLMs side-by-side
- Track reranking model performance
- Visualize metrics across experiments

### 4. **Model Registry**

**Current State**: Models hardcoded in config, no versioning
**With ZenML**: Centralized model registry with staging/production

**Benefits**:

- Version embeddings models (OpenAI, Cohere, etc.)
- Track LLM changes (GPT-4 â†’ GPT-5)
- Gradual rollout of new rerankers
- Model performance metrics attached to versions

### 5. **Production Monitoring**

**Current State**: Basic logging, manual health checks
**With ZenML**: Integrated observability

**Benefits**:

- Pipeline failure alerts
- Data drift detection
- Performance regression detection
- Resource usage tracking

### 6. **Reproducibility**

**Current State**: Config-based, but not guaranteed
**With ZenML**: Hermetic execution environment

**Benefits**:

- Exact reproduction of any pipeline run
- Snapshot all dependencies (code + data + models)
- Debug production issues in development
- Audit trail for compliance

---

## ðŸŽ¨ Proposed Integration Architecture

### High-Level Design

```bash
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ZENML ORCHESTRATION                       â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Ingestion    â”‚  â”‚  Distillation  â”‚  â”‚   Promotion      â”‚  â”‚
â”‚  â”‚   Pipeline     â”‚  â”‚   Pipeline     â”‚  â”‚   Pipeline       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Artifact Store (S3/GCS/Azure)                â”‚   â”‚
â”‚  â”‚  â€¢ Embeddings  â€¢ Chunks  â€¢ Facts  â€¢ Models               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           Metadata Store (PostgreSQL/MySQL)               â”‚   â”‚
â”‚  â”‚  â€¢ Pipeline Runs  â€¢ Lineage  â€¢ Experiments               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      EXISTING SERVICES                            â”‚
â”‚  â€¢ FastAPI  â€¢ LangGraph  â€¢ ChromaDB  â€¢ Background Jobs          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stack Components

**Recommended ZenML Stack**:

```yaml
# Local Development Stack
orchestrator: local
artifact_store: local (filesystem)
metadata_store: sqlite
experiment_tracker: mlflow (local)
model_registry: mlflow (local)

# Production Stack
orchestrator: kubernetes / airflow
artifact_store: s3 / gcs
metadata_store: postgresql
experiment_tracker: mlflow (remote)
model_registry: mlflow (remote)
alerter: slack / pagerduty
```

---

## ðŸ“¦ Pipeline Designs

### Pipeline 1: Document Ingestion

**Purpose**: Ingest raw documents into semantic memory

**Current Implementation**: `IngestDocumentsService.ingest_documents_from_directory()`

**ZenML Pipeline Design**:

```python
from zenml import pipeline, step
from zenml.client import Client
from typing import List, Tuple
import hashlib

@step
def discover_documents(
    directory_path: str,
    supported_extensions: List[str]
) -> List[str]:
    """Step 1: Discover all documents in directory."""
    # Return list of file paths
    # ZenML will cache this based on directory content hash
    pass

@step(enable_cache=True)
def load_and_split_documents(
    file_paths: List[str],
    chunk_size: int = 1024,
    chunk_overlap: int = 200
) -> Tuple[List[str], List[dict]]:
    """Step 2: Load and chunk documents."""
    # Returns: (chunks, metadata)
    # Cached based on file hashes + splitter config
    pass

@step(enable_cache=True)
def generate_embeddings(
    chunks: List[str],
    model_name: str = "text-embedding-3-small"
) -> List[List[float]]:
    """Step 3: Generate embeddings."""
    # Expensive step - caching saves $$$
    # ZenML tracks which embedding model was used
    pass

@step
def store_in_chromadb(
    chunks: List[str],
    embeddings: List[List[float]],
    metadata: List[dict],
    collection_name: str = "documents"
) -> dict:
    """Step 4: Store in ChromaDB."""
    # Returns collection stats
    pass

@pipeline
def ingestion_pipeline(
    directory_path: str,
    chunk_size: int = 1024,
    chunk_overlap: int = 200,
    embedding_model: str = "text-embedding-3-small"
):
    """Full ingestion pipeline with caching."""
    files = discover_documents(directory_path, config.supported_extensions)
    chunks, metadata = load_and_split_documents(files, chunk_size, chunk_overlap)
    embeddings = generate_embeddings(chunks, embedding_model)
    stats = store_in_chromadb(chunks, embeddings, metadata)
    return stats
```

**Benefits**:

- âœ… Automatic caching (skip re-embedding unchanged docs)
- âœ… Track which embedding model was used
- âœ… Compare different chunking strategies
- âœ… Full lineage from raw doc â†’ chunks â†’ embeddings â†’ storage

---

### Pipeline 2: Memory Distillation

**Purpose**: Convert conversation turns into episodic memory facts

**Current Implementation**: `ConversationSummarizer.distill()`

**ZenML Pipeline Design**:

```python
@step
def fetch_recent_turns(
    session_id: str,
    n_turns: int = 5
) -> List[dict]:
    """Step 1: Fetch conversation history."""
    pass

@step(enable_cache=False)  # Always fresh LLM calls
def summarize_conversation(
    turns: List[dict],
    llm_model: str = "gpt-4"
) -> str:
    """Step 2: LLM-based summarization."""
    # Track which LLM was used
    # Log to experiment tracker
    pass

@step
def extract_facts(
    summary: str,
    llm_model: str = "gpt-4"
) -> List[dict]:
    """Step 3: Extract discrete facts."""
    pass

@step
def deduplicate_facts(
    new_facts: List[dict],
    session_id: str
) -> List[dict]:
    """Step 4: Remove duplicates."""
    # Query existing EM facts
    pass

@step
def store_episodic_facts(
    facts: List[dict],
    session_id: str
) -> dict:
    """Step 5: Store in EM collection."""
    pass

@pipeline
def distillation_pipeline(
    session_id: str,
    n_turns: int = 5,
    llm_model: str = "gpt-4"
):
    """Distill conversation into episodic memory."""
    turns = fetch_recent_turns(session_id, n_turns)
    summary = summarize_conversation(turns, llm_model)
    facts = extract_facts(summary, llm_model)
    unique_facts = deduplicate_facts(facts, session_id)
    result = store_episodic_facts(unique_facts, session_id)
    return result
```

**Benefits**:

- âœ… Track which LLM generated which facts
- âœ… Compare different summarization prompts
- âœ… Measure fact extraction quality
- âœ… Lineage: turns â†’ summary â†’ facts â†’ storage

---

### Pipeline 3: Memory Promotion

**Purpose**: Promote high-value EM facts to SM

**Current Implementation**: `MemoryPromoter.run_promotion_cycle()`

**ZenML Pipeline Design**:

```python
@step
def scan_episodic_memory(
    salience_threshold: float = 0.8,
    citation_threshold: int = 5,
    age_days: int = 7
) -> List[dict]:
    """Step 1: Scan EM for promotion candidates."""
    pass

@step
def filter_promotion_candidates(
    facts: List[dict],
    salience_threshold: float,
    citation_threshold: int,
    age_days: int
) -> List[dict]:
    """Step 2: Apply promotion criteria."""
    pass

@step
def transform_for_semantic_memory(
    facts: List[dict]
) -> List[dict]:
    """Step 3: Transform metadata for SM."""
    # Remove session_id, adjust metadata
    pass

@step
def promote_to_semantic_memory(
    facts: List[dict]
) -> dict:
    """Step 4: Copy to SM collection."""
    pass

@step
def cleanup_promoted_facts(
    fact_ids: List[str]
) -> dict:
    """Step 5: Remove from EM."""
    pass

@pipeline
def promotion_pipeline(
    salience_threshold: float = 0.8,
    citation_threshold: int = 5,
    age_days: int = 7
):
    """Promote episodic facts to semantic memory."""
    candidates = scan_episodic_memory(
        salience_threshold,
        citation_threshold,
        age_days
    )
    filtered = filter_promotion_candidates(
        candidates,
        salience_threshold,
        citation_threshold,
        age_days
    )
    transformed = transform_for_semantic_memory(filtered)
    result = promote_to_semantic_memory(transformed)
    cleanup = cleanup_promoted_facts([f["id"] for f in transformed])
    return result
```

**Benefits**:
- âœ… Experiment with different promotion thresholds
- âœ… Track promotion rates over time
- âœ… A/B test criteria changes
- âœ… Rollback if promotion degrades quality

---

### Pipeline 4: Evaluation Pipeline (NEW)

**Purpose**: Evaluate RAG quality with metrics

**ZenML Pipeline Design**:

```python
@step
def load_eval_dataset() -> List[dict]:
    """Load evaluation Q&A pairs."""
    pass

@step
def run_rag_queries(
    eval_dataset: List[dict],
    config: dict
) -> List[dict]:
    """Run RAG system on eval queries."""
    pass

@step
def compute_metrics(
    predictions: List[dict],
    ground_truth: List[dict]
) -> dict:
    """Compute RAGAS metrics."""
    # Faithfulness, Answer Relevancy, Context Recall
    pass

@pipeline
def evaluation_pipeline(
    eval_dataset_path: str,
    model_version: str
):
    """Evaluate RAG system quality."""
    dataset = load_eval_dataset()
    predictions = run_rag_queries(dataset, {"model": model_version})
    metrics = compute_metrics(predictions, dataset)
    return metrics
```

**Benefits**:
- âœ… Track quality metrics over time
- âœ… Compare model versions
- âœ… Catch regressions before production
- âœ… Automated quality gates

---

## ðŸ—„ï¸ Artifact Management Strategy

### Artifact Types

**1. Data Artifacts**:
- `RawDocuments`: Original files (PDF, TXT, etc.)
- `DocumentChunks`: Split text with metadata
- `EmbeddingVectors`: Cached embeddings
- `ConversationTurns`: Chat history
- `EpisodicFacts`: Distilled memories
- `EvalDatasets`: Test Q&A pairs

**2. Model Artifacts**:
- `EmbeddingModel`: text-embedding-3-small, etc.
- `LLMModel`: GPT-4, Claude, etc.
- `RerankerModel`: Cross-encoder weights
- `IntentClassifier`: Fine-tuned classifier (if custom)

**3. Metrics Artifacts**:
- `IngestionStats`: Docs processed, chunks created
- `DistillationMetrics`: Facts extracted, dedup rate
- `PromotionMetrics`: Promotion rate, criteria stats
- `RAGMetrics`: RAGAS scores, latency, cost

### Artifact Lineage Example

```bash
RawDocument_v1 (PDF)
  â””â”€> DocumentChunks_v1 (1024 size, 200 overlap)
      â””â”€> EmbeddingVectors_v1 (text-embedding-3-small)
          â””â”€> ChromaDB_SM_v1
              â””â”€> RAGResponse_v1
                  â””â”€> ConversationTurn_v1
                      â””â”€> EpisodicFact_v1
                          â””â”€> [Promotion] â†’ ChromaDB_SM_v2
```

**Queries Enabled**:
- "Which documents contributed to this fact?"
- "When did we change embedding models?"
- "What chunking strategy produced the best metrics?"

### Caching Strategy

**Cache These Steps** (expensive):
- âœ… `generate_embeddings`: $0.0001/token adds up
- âœ… `load_and_split_documents`: I/O intensive
- âœ… `scan_episodic_memory`: Large DB scans

**Don't Cache** (must be fresh):
- âŒ `fetch_recent_turns`: Conversation state
- âŒ `summarize_conversation`: LLM creativity
- âŒ `run_rag_queries`: User queries

---

## ðŸ“Š Experiment Tracking

### Tracked Experiments

**1. Chunking Strategy Experiments**:
```python
# Experiment: Optimal chunk size
for chunk_size in [512, 1024, 2048]:
    for overlap in [100, 200, 400]:
        run = ingestion_pipeline(
            chunk_size=chunk_size,
            chunk_overlap=overlap
        )
        # ZenML logs: chunk_size, overlap, retrieval_accuracy
```

**2. Embedding Model Comparison**:
```python
# Experiment: Which embeddings work best?
models = [
    "text-embedding-3-small",
    "text-embedding-3-large",
    "text-embedding-ada-002"
]
for model in models:
    run = ingestion_pipeline(embedding_model=model)
    eval = evaluation_pipeline(model_version=model)
    # Compare: cost, latency, accuracy
```

**3. Promotion Threshold Tuning**:
```python
# Experiment: Optimal promotion criteria
for salience in [0.6, 0.7, 0.8, 0.9]:
    for citations in [3, 5, 7, 10]:
        run = promotion_pipeline(
            salience_threshold=salience,
            citation_threshold=citations
        )
        # Track: promotion_rate, SM_quality
```

### MLflow Integration

```python
# ZenML automatically logs to MLflow
from zenml.integrations.mlflow import mlflow_experiment_tracker

@pipeline(
    experiment_tracker=mlflow_experiment_tracker,
    enable_cache=True
)
def my_pipeline(...):
    # All metrics auto-logged
    pass

# View in MLflow UI
# Compare runs, visualize metrics, download artifacts
```

---

## ðŸ·ï¸ Model Registry Integration

### Model Versioning Strategy

**Embedding Models**:
```python
from zenml.integrations.mlflow import MLFlowModelDeployer

# Register embedding model
model_registry.register_model(
    name="text-embedding",
    version="3-small",
    model_type="openai",
    metadata={
        "dimensions": 1536,
        "cost_per_1k_tokens": 0.00002,
        "max_tokens": 8191
    },
    stage="production"
)
```

**LLM Models**:
```python
# Track LLM changes
model_registry.register_model(
    name="chat-llm",
    version="gpt-4-turbo-2024",
    model_type="openai",
    metadata={
        "context_window": 128000,
        "cost_per_1k_input": 0.01,
        "cost_per_1k_output": 0.03
    },
    stage="staging"
)

# Gradual rollout
# 10% traffic â†’ staging
# 90% traffic â†’ production
```

**Reranker Models**:
```python
# Version cross-encoder models
model_registry.register_model(
    name="reranker",
    version="ms-marco-v1",
    model_type="cross-encoder",
    metadata={
        "model_path": "cross-encoder/ms-marco-MiniLM-L-6-v2",
        "max_length": 512
    },
    stage="production"
)
```

### Model Lifecycle

```
Development â†’ Staging â†’ Production â†’ Archived
     â†“           â†“          â†“            â†“
  Experiment   A/B Test   Serving    Deprecated
```

---

## ðŸš¢ Deployment Strategy

### Deployment Scenarios

**Scenario 1: Pipeline as Batch Job**
```python
# Run ingestion daily
from zenml.pipelines import Schedule

ingestion_pipeline.run(
    schedule=Schedule(cron_expression="0 2 * * *")  # 2 AM daily
)
```

**Scenario 2: Pipeline as API Endpoint**
```python
# FastAPI integration
@app.post("/ingest")
async def trigger_ingestion(directory: str):
    run = ingestion_pipeline.run(
        directory_path=directory
    )
    return {"run_id": run.id, "status": "started"}
```

**Scenario 3: Event-Driven Pipelines**
```python
# Trigger on S3 upload
from zenml.integrations.aws import s3_event_source

@pipeline(trigger=s3_event_source("my-bucket"))
def auto_ingest_pipeline(s3_path: str):
    # Automatically ingest new documents
    pass
```

### Production Stack

**Recommended Setup**:
1. **Orchestrator**: Kubernetes (scalable)
2. **Artifact Store**: S3 (durable, versioned)
3. **Metadata Store**: PostgreSQL (relational)
4. **Experiment Tracker**: MLflow (remote)
5. **Alerter**: Slack (notifications)
6. **Secrets**: AWS Secrets Manager

---

## ðŸ›¤ï¸ Migration Path

### Phase 1: Foundation (Week 1-2)

**Goals**: Set up ZenML, migrate ingestion pipeline

**Tasks**:
1. Install ZenML: `pip install zenml[server]`
2. Initialize ZenML: `zenml init`
3. Create local stack
4. Convert ingestion service to ZenML pipeline
5. Test with sample documents
6. Validate artifact caching works

**Success Criteria**:
- âœ… Ingestion pipeline running in ZenML
- âœ… Artifacts stored and cached
- âœ… MLflow tracking working

---

### Phase 2: Memory Pipelines (Week 3-4)

**Goals**: Migrate distillation and promotion

**Tasks**:
1. Convert distillation to ZenML pipeline
2. Convert promotion to ZenML pipeline
3. Set up scheduled runs (cron)
4. Integrate with existing FastAPI
5. Add experiment tracking for prompts

**Success Criteria**:
- âœ… Distillation pipeline automated
- âœ… Promotion pipeline scheduled (daily)
- âœ… Lineage tracked end-to-end

---

### Phase 3: Evaluation & Optimization (Week 5-6)

**Goals**: Add evaluation, optimize caching

**Tasks**:
1. Create evaluation pipeline
2. Build RAGAS metrics integration
3. Set up A/B testing framework
4. Optimize artifact caching strategy
5. Add model registry versioning

**Success Criteria**:
- âœ… Automated quality evaluation
- âœ… Experiment comparison dashboard
- âœ… Model versions tracked

---

### Phase 4: Production Deployment (Week 7-8)

**Goals**: Deploy to production stack

**Tasks**:
1. Set up Kubernetes orchestrator
2. Configure S3 artifact store
3. Deploy PostgreSQL metadata store
4. Set up remote MLflow
5. Configure alerting (Slack)
6. Add monitoring dashboards

**Success Criteria**:
- âœ… Production stack deployed
- âœ… Pipelines running in Kubernetes
- âœ… Alerts configured
- âœ… Full observability

---

## ðŸ’¡ Best Practices

### 1. **Step Granularity**

**Good** (atomic steps):
```python
@step
def load_documents(paths: List[str]) -> List[str]:
    pass

@step
def split_documents(docs: List[str]) -> List[str]:
    pass
```

**Bad** (monolithic step):
```python
@step
def load_split_embed_store_all():
    # Too much in one step
    # Can't cache intermediate results
    pass
```

### 2. **Artifact Naming**

**Good** (descriptive):
```python
@step(output_materializers=TextMaterializer)
def extract_facts(...) -> Annotated[List[dict], "episodic_facts"]:
    pass
```

**Bad** (generic):
```python
@step
def extract_facts(...) -> List[dict]:
    pass
```

### 3. **Caching Strategy**

```python
# Expensive, deterministic â†’ cache
@step(enable_cache=True)
def generate_embeddings(...):
    pass

# Cheap, non-deterministic â†’ don't cache
@step(enable_cache=False)
def fetch_recent_turns(...):
    pass
```

### 4. **Parameter Management**

```python
# Use config files for reusability
from zenml.config import ResourceSettings

@pipeline(
    settings={
        "resources": ResourceSettings(
            cpu_count=4,
            memory="8GB"
        )
    }
)
def ingestion_pipeline(...):
    pass
```

### 5. **Error Handling**

```python
@step(
    retry=StepRetryConfig(max_retries=3, delay=60),
    on_failure=send_slack_alert
)
def call_external_api(...):
    pass
```

---

## ðŸ“… Implementation Roadmap

### Month 1: Foundation
- âœ… Week 1: Setup + ingestion pipeline
- âœ… Week 2: Testing + validation
- âœ… Week 3: Distillation pipeline
- âœ… Week 4: Promotion pipeline

### Month 2: Enhancement
- âœ… Week 5: Evaluation pipeline
- âœ… Week 6: Experiment tracking
- âœ… Week 7: Model registry
- âœ… Week 8: Optimization

### Month 3: Production
- âœ… Week 9: Production stack setup
- âœ… Week 10: Kubernetes deployment
- âœ… Week 11: Monitoring + alerts
- âœ… Week 12: Documentation + training

---

## ðŸŽ“ Learning Resources

### Official Documentation
- [ZenML Docs](https://docs.zenml.io/)
- [ZenML Examples](https://github.com/zenml-io/zenml/tree/main/examples)
- [ZenML Discord](https://zenml.io/slack)

### Relevant Examples
- [RAG Pipeline Example](https://github.com/zenml-io/zenml-projects/tree/main/llm-complete-guide)
- [LLM Finetuning](https://github.com/zenml-io/zenml-projects/tree/main/llm-finetuning)
- [Experiment Tracking](https://docs.zenml.io/user-guide/starter-guide/track-ml-models)

---

## ðŸ”š Conclusion

### Summary

Integrating ZenML into your memory-aware RAG system will provide:

1. **Production-grade MLOps**: Versioning, lineage, reproducibility
2. **Faster iteration**: Caching, experiment tracking, A/B testing
3. **Better observability**: Full pipeline visibility, alerting
4. **Scalability**: Kubernetes orchestration, distributed execution
5. **Team collaboration**: Shared pipelines, model registry, dashboards

### Next Steps

1. **Install ZenML**: `pip install "zenml[server]"`
2. **Review examples**: Clone zenml-projects repo
3. **Start small**: Convert ingestion pipeline first
4. **Iterate**: Add features incrementally
5. **Measure**: Track time saved, quality improvements

### Success Metrics

Track these KPIs to measure ZenML impact:

- **Experiment velocity**: Experiments/week (target: 3x increase)
- **Debugging time**: Hours to diagnose issues (target: 60% reduction)
- **Reproducibility**: Successful re-runs (target: 100%)
- **Pipeline reliability**: Success rate (target: >95%)
- **Cost savings**: Reduced re-computation via caching (target: 40%)

---

**Document Status**: Ready for Review  
**Recommended Next Action**: Proof of concept with ingestion pipeline  
**Estimated Implementation Time**: 8-12 weeks (phased approach)

