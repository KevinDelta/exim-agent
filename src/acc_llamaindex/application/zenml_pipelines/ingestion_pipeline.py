"""ZenML pipeline for document ingestion."""

from pathlib import Path
from typing import List, Tuple, Dict, Any
from loguru import logger

from zenml import pipeline, step

ZENML_AVAILABLE = True

from acc_llamaindex.config import config
from acc_llamaindex.infrastructure.db.chroma_client import chroma_client
from acc_llamaindex.application.ingest_documents_service.service import ingest_service


@step
def discover_documents(
    directory_path: str,
    supported_extensions: List[str]
) -> List[str]:
    """
    Step 1: Discover all documents in directory.
    
    Args:
        directory_path: Path to directory containing documents
        supported_extensions: List of supported file extensions
        
    Returns:
        List of file paths to process
    """
    logger.info(f"Discovering documents in {directory_path}")
    
    directory = Path(directory_path)
    if not directory.exists():
        logger.error(f"Directory not found: {directory_path}")
        return []
    
    file_paths = []
    for ext in supported_extensions:
        for file_path in directory.rglob(f"*{ext}"):
            if file_path.is_file():
                file_paths.append(str(file_path))
    
    logger.info(f"Found {len(file_paths)} documents")
    return file_paths


@step(enable_cache=True)
def load_and_split_documents(
    file_paths: List[str],
    chunk_size: int = 1024,
    chunk_overlap: int = 200
) -> Tuple[List[str], List[Dict[str, Any]]]:
    """
    Step 2: Load and chunk documents.
    
    Cached based on file hashes + splitter config to avoid reprocessing.
    
    Args:
        file_paths: List of file paths to process
        chunk_size: Size of text chunks
        chunk_overlap: Overlap between chunks
        
    Returns:
        Tuple of (chunks, metadata)
    """
    logger.info(f"Loading and splitting {len(file_paths)} documents")
    
    all_chunks = []
    all_metadata = []
    
    for file_path in file_paths:
        try:
            # Use existing service's loader logic
            loader = ingest_service._get_loader_for_extension(file_path)
            loaded_docs = loader.load()
            
            # Split documents
            splits = ingest_service.text_splitter.split_documents(loaded_docs)
            
            for split in splits:
                all_chunks.append(split.page_content)
                all_metadata.append(split.metadata)
                
        except Exception as e:
            logger.error(f"Failed to process {file_path}: {e}")
            continue
    
    logger.info(f"Created {len(all_chunks)} chunks from {len(file_paths)} documents")
    return all_chunks, all_metadata


@step(enable_cache=True)
def generate_embeddings(
    chunks: List[str],
    model_name: str = "text-embedding-3-small"
) -> List[List[float]]:
    """
    Step 3: Generate embeddings.
    
    Expensive step - caching saves time and money.
    ZenML tracks which embedding model was used.
    
    Args:
        chunks: List of text chunks
        model_name: Embedding model identifier
        
    Returns:
        List of embedding vectors
    """
    logger.info(f"Generating embeddings for {len(chunks)} chunks using {model_name}")
    
    # Note: ChromaDB's vector store handles embedding generation internally
    # This step is a placeholder for explicit embedding generation
    # In production, you'd use the embedding model directly
    
    logger.info("Embeddings will be generated by ChromaDB on storage")
    return []  # ChromaDB handles embeddings


@step
def store_in_chromadb(
    chunks: List[str],
    embeddings: List[List[float]],
    metadata: List[Dict[str, Any]],
    collection_name: str = "documents"
) -> Dict[str, Any]:
    """
    Step 4: Store chunks in ChromaDB.
    
    Args:
        chunks: Text chunks to store
        embeddings: Pre-computed embeddings (or empty if ChromaDB generates)
        metadata: Metadata for each chunk
        collection_name: Target collection name
        
    Returns:
        Collection statistics
    """
    logger.info(f"Storing {len(chunks)} chunks in ChromaDB collection: {collection_name}")
    
    try:
        # Get vector store
        vector_store = chroma_client.get_vector_store()
        
        # Add documents (ChromaDB generates embeddings if needed)
        from langchain_core.documents import Document
        docs = [Document(page_content=chunk, metadata=meta) 
                for chunk, meta in zip(chunks, metadata)]
        vector_store.add_documents(docs)
        
        # Get collection stats
        stats = chroma_client.get_collection_stats()
        
        logger.info(f"Successfully stored {len(chunks)} chunks")
        return {
            "chunks_stored": len(chunks),
            "collection_stats": stats,
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"Failed to store chunks: {e}")
        return {
            "chunks_stored": 0,
            "error": str(e),
            "status": "error"
        }


@pipeline
def ingestion_pipeline(
    directory_path: str,
    chunk_size: int = 1024,
    chunk_overlap: int = 200,
    embedding_model: str = "text-embedding-3-small"
) -> Dict[str, Any]:
    """
    Full document ingestion pipeline with caching.
    
    Benefits:
    - Automatic caching (skip re-embedding unchanged docs)
    - Track which embedding model was used
    - Compare different chunking strategies
    - Full lineage from raw doc → chunks → embeddings → storage
    
    Args:
        directory_path: Path to documents directory
        chunk_size: Size of text chunks (default: 1024)
        chunk_overlap: Overlap between chunks (default: 200)
        embedding_model: Model to use for embeddings
        
    Returns:
        Pipeline execution statistics
    """
    # Step 1: Discover documents
    files = discover_documents(
        directory_path=directory_path,
        supported_extensions=config.supported_file_extensions
    )
    
    # Step 2: Load and split
    chunks, metadata = load_and_split_documents(
        file_paths=files,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    
    # Step 3: Generate embeddings (or let ChromaDB handle it)
    embeddings = generate_embeddings(
        chunks=chunks,
        model_name=embedding_model
    )
    
    # Step 4: Store in ChromaDB
    stats = store_in_chromadb(
        chunks=chunks,
        embeddings=embeddings,
        metadata=metadata,
        collection_name="documents"
    )
    
    return stats


# Function to run the pipeline (for backwards compatibility)
def run_ingestion_pipeline(
    directory_path: str | None = None,
    **kwargs
) -> Dict[str, Any]:
    """
    Run the ingestion pipeline.
    
    Args:
        directory_path: Optional directory path (uses config default if None)
        **kwargs: Additional pipeline parameters
        
    Returns:
        Pipeline execution results
    """
    if not ZENML_AVAILABLE:
        logger.error("ZenML is not available - falling back to regular ingestion service")
        # Fallback to existing service
        result = ingest_service.ingest_documents_from_directory(directory_path)
        return {
            "status": "success" if result.success else "error",
            "documents_processed": result.documents_processed,
            "message": result.message
        }
    
    path = directory_path or str(config.documents_path)
    return ingestion_pipeline(directory_path=path, **kwargs)
