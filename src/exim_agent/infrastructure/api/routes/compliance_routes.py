"""Compliance Pulse API routes."""

from datetime import datetime, timedelta
from fastapi import APIRouter, HTTPException, status
from loguru import logger

from exim_agent.application.compliance_service.service import compliance_service
from exim_agent.infrastructure.db.compliance_collections import ComplianceCollections
from exim_agent.infrastructure.api.models import (
    AskRequest,
    AskResponse,
    SnapshotRequest,
    SnapshotResponse,
    WeeklyPulseResponse,
)

router = APIRouter(prefix="/compliance", tags=["compliance"])

# Initialize compliance collections
compliance_collections = ComplianceCollections()


# API Endpoints

@router.post("/snapshot", response_model=SnapshotResponse)
async def generate_snapshot(request: SnapshotRequest) -> SnapshotResponse:
    """
    Generate compliance snapshot for a SKU + Lane combination.
    
    Returns 4 tiles:
    - HTS Classification & Requirements
    - Sanctions Screening
    - Refusal History & Risks
    - Relevant CBP Rulings
    
    Each tile includes risk level, status, and actionable insights.
    """
    try:
        logger.info(f"Snapshot request: {request.client_id}/{request.sku_id}/{request.lane_id}")
        
        # Initialize service if needed
        if compliance_service.graph is None:
            compliance_service.initialize()
        
        # Generate snapshot
        result = compliance_service.snapshot(
            client_id=request.client_id,
            sku_id=request.sku_id,
            lane_id=request.lane_id
        )
        
        # Add metadata
        metadata = {
            "generated_at": datetime.utcnow().isoformat(),
            "client_id": request.client_id,
            "sku_id": request.sku_id,
            "lane_id": request.lane_id
        }
        
        return SnapshotResponse(
            success=result.get("success", False),
            snapshot=result.get("snapshot"),
            citations=result.get("citations"),
            error=result.get("error"),
            metadata=metadata
        )
        
    except Exception as e:
        logger.error(f"Snapshot generation failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate snapshot: {str(e)}"
        )


@router.get("/pulse/{client_id}/weekly", response_model=WeeklyPulseResponse)
async def get_weekly_pulse(
    client_id: str,
    limit: int = 1,
    requires_action_only: bool = False
) -> WeeklyPulseResponse:
    """
    Get weekly compliance pulse digest(s) for a client from Supabase.
    
    Retrieves stored weekly pulse digests generated by the ZenML pipeline.
    
    Query Parameters:
    - limit: Number of digests to return (default: 1 = latest only)
    - requires_action_only: Only return digests with action items (default: false)
    
    Returns:
    - Summary of new/changed compliance requirements
    - Delta analysis (what changed this week vs last week)
    - Prioritized action items
    - Trend analysis
    """
    try:
        logger.info(f"Weekly pulse request for client: {client_id} (limit={limit}, action_only={requires_action_only})")
        
        # Retrieve digest(s) from Supabase
        from exim_agent.infrastructure.db.supabase_client import supabase_client
        
        digests = supabase_client.get_weekly_pulse_digests(
            client_id=client_id,
            limit=limit,
            requires_action_only=requires_action_only
        )
        
        if not digests:
            logger.warning(f"No weekly pulse digests found for client: {client_id}")
            return WeeklyPulseResponse(
                success=False,
                client_id=client_id,
                period_start=None,
                period_end=None,
                summary={
                    "total_sku_lanes": 0,
                    "high_priority_changes": 0,
                    "medium_priority_changes": 0,
                    "low_priority_changes": 0,
                    "new_sanctions": 0,
                    "new_refusals": 0,
                    "policy_updates": 0
                },
                changes=[],
                error="No weekly pulse digests found. Run the weekly_pulse_pipeline first."
            )
        
        # Return the latest digest (or all if limit > 1)
        latest_digest = digests[0]
        digest_data = latest_digest.get("digest_data", {})
        
        # Extract summary
        summary = digest_data.get("summary", {})
        
        # Extract changes with proper structure
        changes = []
        for change in digest_data.get("top_changes", []):
            changes.append({
                "sku_id": change.get("sku_lane_key", "").split(":")[0] if ":" in change.get("sku_lane_key", "") else "unknown",
                "lane_id": change.get("sku_lane_key", "").split(":")[1] if ":" in change.get("sku_lane_key", "") else "unknown",
                "change_type": change.get("change_type", "unknown"),
                "priority": change.get("priority", "low"),
                "description": change.get("description", ""),
                "timestamp": change.get("timestamp", "")
            })
        
        return WeeklyPulseResponse(
            success=True,
            client_id=client_id,
            period_start=latest_digest.get("period_start"),
            period_end=latest_digest.get("period_end"),
            summary=summary,
            changes=changes,
            error=None,
            metadata={
                "digest_id": latest_digest.get("id"),
                "generated_at": latest_digest.get("generated_at"),
                "requires_action": latest_digest.get("requires_action"),
                "status": latest_digest.get("status"),
                "total_digests_available": len(digests)
            }
        )
        
    except Exception as e:
        logger.error(f"Weekly pulse retrieval failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve weekly pulse: {str(e)}"
        )


@router.get("/pulse/{client_id}/latest", response_model=WeeklyPulseResponse)
async def get_latest_pulse(
    client_id: str
) -> WeeklyPulseResponse:
    """
    Get the most recent compliance pulse digest (weekly or daily) for a client.
    
    Retrieves the single most recent digest regardless of period type.
    This is useful for displaying the current compliance status on dashboards.
    
    Returns:
    - Most recent digest with full summary and changes
    - Metadata indicating whether it's a weekly or daily digest
    - Period information (start/end dates)
    """
    try:
        logger.info(f"Latest pulse request for client: {client_id}")
        
        # Retrieve latest digest from Supabase
        from exim_agent.infrastructure.db.supabase_client import supabase_client
        
        latest_digest = supabase_client.get_latest_digest(client_id)
        
        if not latest_digest:
            logger.warning(f"No pulse digests found for client: {client_id}")
            return WeeklyPulseResponse(
                success=False,
                client_id=client_id,
                period_start=None,
                period_end=None,
                summary={
                    "total_sku_lanes": 0,
                    "high_priority_changes": 0,
                    "medium_priority_changes": 0,
                    "low_priority_changes": 0,
                    "new_sanctions": 0,
                    "new_refusals": 0,
                    "policy_updates": 0
                },
                changes=[],
                error="No pulse digests found. Run the weekly_pulse_pipeline first."
            )
        
        # Extract digest data
        digest_data = latest_digest.get("digest_data", {})
        
        # Extract summary
        summary = digest_data.get("summary", {})
        
        # Extract changes with proper structure
        changes = []
        for change in digest_data.get("top_changes", []):
            changes.append({
                "sku_id": change.get("sku_lane_key", "").split(":")[0] if ":" in change.get("sku_lane_key", "") else "unknown",
                "lane_id": change.get("sku_lane_key", "").split(":")[1] if ":" in change.get("sku_lane_key", "") else "unknown",
                "change_type": change.get("change_type", "unknown"),
                "priority": change.get("priority", "low"),
                "description": change.get("description", ""),
                "timestamp": change.get("timestamp", "")
            })
        
        # Determine period type (daily vs weekly)
        try:
            period_start = datetime.fromisoformat(latest_digest["period_start"].replace('Z', '+00:00'))
            period_end = datetime.fromisoformat(latest_digest["period_end"].replace('Z', '+00:00'))
            period_duration = (period_end - period_start).days
            period_type = "daily" if period_duration <= 1 else "weekly"
        except (ValueError, KeyError):
            period_type = "unknown"
        
        return WeeklyPulseResponse(
            success=True,
            client_id=client_id,
            period_start=latest_digest.get("period_start"),
            period_end=latest_digest.get("period_end"),
            summary=summary,
            changes=changes,
            error=None,
            metadata={
                "digest_id": latest_digest.get("id"),
                "generated_at": latest_digest.get("generated_at"),
                "requires_action": latest_digest.get("requires_action"),
                "status": latest_digest.get("status"),
                "period_type": period_type,
                "is_latest": True
            }
        )
        
    except Exception as e:
        logger.error(f"Latest pulse retrieval failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve latest pulse: {str(e)}"
        )


@router.get("/pulse/{client_id}/daily", response_model=WeeklyPulseResponse)
async def get_daily_pulse(
    client_id: str,
    limit: int = 1,
    requires_action_only: bool = False,
    start_date: str = None,
    end_date: str = None
) -> WeeklyPulseResponse:
    """
    Get daily compliance pulse digest(s) for a client from Supabase.
    
    Retrieves stored daily pulse digests (period_days=1) generated by the ZenML pipeline.
    
    Query Parameters:
    - limit: Number of digests to return (default: 1 = latest only)
    - requires_action_only: Only return digests with action items (default: false)
    - start_date: Filter digests from this date (ISO format: YYYY-MM-DD)
    - end_date: Filter digests up to this date (ISO format: YYYY-MM-DD)
    
    Returns:
    - Summary of new/changed compliance requirements
    - Delta analysis (what changed today vs yesterday)
    - Prioritized action items
    - Daily trend analysis
    """
    try:
        logger.info(f"Daily pulse request for client: {client_id} (limit={limit}, action_only={requires_action_only}, start_date={start_date}, end_date={end_date})")
        
        # Retrieve digest(s) from Supabase
        from exim_agent.infrastructure.db.supabase_client import supabase_client
        
        # Get digests and filter for daily (period_days=1)
        digests = supabase_client.get_weekly_pulse_digests(
            client_id=client_id,
            limit=limit * 10,  # Get more to filter for daily
            requires_action_only=requires_action_only
        )
        
        # Filter for daily digests (period_end - period_start <= 1 day)
        daily_digests = []
        for digest in digests:
            try:
                period_start = datetime.fromisoformat(digest["period_start"].replace('Z', '+00:00'))
                period_end = datetime.fromisoformat(digest["period_end"].replace('Z', '+00:00'))
                period_duration = (period_end - period_start).days
                
                # Filter for daily digests (1 day period)
                if period_duration <= 1:
                    # Apply date range filters if provided
                    if start_date:
                        filter_start = datetime.fromisoformat(start_date)
                        if period_end.date() < filter_start.date():
                            continue
                    
                    if end_date:
                        filter_end = datetime.fromisoformat(end_date)
                        if period_end.date() > filter_end.date():
                            continue
                    
                    daily_digests.append(digest)
                    
                    if len(daily_digests) >= limit:
                        break
            except (ValueError, KeyError) as e:
                logger.warning(f"Failed to parse digest dates: {e}")
                continue
        
        if not daily_digests:
            logger.warning(f"No daily pulse digests found for client: {client_id}")
            return WeeklyPulseResponse(
                success=False,
                client_id=client_id,
                period_start=None,
                period_end=None,
                summary={
                    "total_sku_lanes": 0,
                    "high_priority_changes": 0,
                    "medium_priority_changes": 0,
                    "low_priority_changes": 0,
                    "new_sanctions": 0,
                    "new_refusals": 0,
                    "policy_updates": 0
                },
                changes=[],
                error="No daily pulse digests found. Run the weekly_pulse_pipeline with period_days=1 first."
            )
        
        # Return the latest digest (or all if limit > 1)
        latest_digest = daily_digests[0]
        digest_data = latest_digest.get("digest_data", {})
        
        # Extract summary
        summary = digest_data.get("summary", {})
        
        # Extract changes with proper structure
        changes = []
        for change in digest_data.get("top_changes", []):
            changes.append({
                "sku_id": change.get("sku_lane_key", "").split(":")[0] if ":" in change.get("sku_lane_key", "") else "unknown",
                "lane_id": change.get("sku_lane_key", "").split(":")[1] if ":" in change.get("sku_lane_key", "") else "unknown",
                "change_type": change.get("change_type", "unknown"),
                "priority": change.get("priority", "low"),
                "description": change.get("description", ""),
                "timestamp": change.get("timestamp", "")
            })
        
        return WeeklyPulseResponse(
            success=True,
            client_id=client_id,
            period_start=latest_digest.get("period_start"),
            period_end=latest_digest.get("period_end"),
            summary=summary,
            changes=changes,
            error=None,
            metadata={
                "digest_id": latest_digest.get("id"),
                "generated_at": latest_digest.get("generated_at"),
                "requires_action": latest_digest.get("requires_action"),
                "status": latest_digest.get("status"),
                "total_digests_available": len(daily_digests),
                "period_type": "daily"
            }
        )
        
    except Exception as e:
        logger.error(f"Daily pulse retrieval failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve daily pulse: {str(e)}"
        )


@router.post("/ask", response_model=AskResponse)
async def ask_compliance_question(request: AskRequest) -> AskResponse:
    """
    Answer compliance questions using RAG.
    
    Queries:
    - HTS notes and requirements
    - CBP rulings database
    - Refusal summaries
    - Policy snippets
    - Client-specific context (via mem0)
    
    Returns natural language answer with citations.
    """
    try:
        logger.info(f"Q&A request from {request.client_id}: {request.question}")
        
        # Initialize service if needed
        if compliance_service.graph is None:
            compliance_service.initialize()
        
        # Process question
        result = compliance_service.ask(
            client_id=request.client_id,
            question=request.question,
            sku_id=request.sku_id,
            lane_id=request.lane_id
        )
        
        return AskResponse(
            success=result.get("success", False),
            answer=result.get("answer"),
            citations=result.get("citations"),
            question=request.question,
            error=result.get("error")
        )
        
    except Exception as e:
        logger.error(f"Q&A request failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to process question: {str(e)}"
        )


@router.get("/collections/status")
async def get_collections_status():
    """
    Get status of compliance collections.
    
    Returns document counts and collection metadata.
    """
    try:
        # Initialize if needed
        if not compliance_collections._initialized:
            compliance_collections.initialize()
        
        stats = compliance_collections.get_stats()
        
        return {
            "success": True,
            "collections": stats,
            "total_documents": sum(s.get("count", 0) for s in stats.values() if isinstance(s, dict))
        }
        
    except Exception as e:
        logger.error(f"Failed to get collections status: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get collections status: {str(e)}"
        )


@router.post("/collections/seed")
async def seed_compliance_data():
    """
    Seed compliance collections with sample data.
    
    Only for development/testing. Should be protected in production.
    """
    try:
        logger.info("Seeding compliance collections...")
        
        # Initialize if needed
        if not compliance_collections._initialized:
            compliance_collections.initialize()
        
        # Seed data
        compliance_collections.seed_sample_data()
        
        # Get updated stats
        stats = compliance_collections.get_stats()
        
        return {
            "success": True,
            "message": "Collections seeded successfully",
            "collections": stats
        }
        
    except Exception as e:
        logger.error(f"Failed to seed collections: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to seed collections: {str(e)}"
        )
