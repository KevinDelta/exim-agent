{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain v1 RAG Application Examples\n",
    "\n",
    "This notebook demonstrates usage patterns for the refactored RAG application using **LangChain v1**.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Initialization\n",
    "2. Document Ingestion Patterns\n",
    "3. RAG Chat with Agents\n",
    "4. Advanced Agent Patterns\n",
    "5. Retriever Customization\n",
    "6. Conversation Memory\n",
    "7. Structured Outputs\n",
    "8. Error Handling and Observability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinknox/coding/acc-llamaindex/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2025-10-18 19:00:19.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mIngestDocumentsService initialized with documents_path: /Users/kevinknox/coding/acc-llamaindex/data/documents\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:19.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.reranking_service.service\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mRerankingService initialized\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:19.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.evaluation_service.service\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mEvaluationService initialized\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:19.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mChatService initialized\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Add project root to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Verify imports\n",
    "from acc_llamaindex.config import config\n",
    "from acc_llamaindex.infrastructure.llm_providers.langchain_provider import get_llm, get_embeddings, reset_llm\n",
    "from acc_llamaindex.infrastructure.db.chroma_client import chroma_client\n",
    "from acc_llamaindex.application.ingest_documents_service.service import ingest_service\n",
    "from acc_llamaindex.application.chat_service.service import chat_service\n",
    "from acc_llamaindex.infrastructure.llm_providers import langchain_provider, anthropic_provider \n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Ingestion Patterns\n",
    "\n",
    "### Pattern 1: Basic Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 19:00:29.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36m_initialize_llm\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mInitializing LLM with provider: openai\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:29.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mInitializing ChatOpenAI with model: gpt-5-nano-2025-08-07\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:30.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mChatOpenAI initialized successfully\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:30.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36m_initialize_embeddings\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mInitializing embeddings with provider: openai\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:30.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_embeddings\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mInitializing OpenAIEmbeddings with model: text-embedding-3-small\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:30.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_embeddings\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mOpenAIEmbeddings initialized successfully\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:30.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.db.chroma_client\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mInitializing ChromaDB at /Users/kevinknox/coding/acc-llamaindex/data/chroma_db\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:30.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.db.chroma_client\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mChromaDB initialized successfully with collection: documents\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents path: /Users/kevinknox/coding/acc-llamaindex/data/documents\n",
      "ChromaDB path: /Users/kevinknox/coding/acc-llamaindex/data/chroma_db\n",
      "Chunk size: 1024, overlap: 200\n"
     ]
    }
   ],
   "source": [
    "# Initialize services\n",
    "get_llm()\n",
    "get_embeddings()\n",
    "chroma_client.initialize()\n",
    "\n",
    "print(f\"Documents path: {config.documents_path}\")\n",
    "print(f\"ChromaDB path: {config.chroma_db_path}\")\n",
    "print(f\"Chunk size: {config.chunk_size}, overlap: {config.chunk_overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 19:00:36.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mStarting document ingestion from: /Users/kevinknox/coding/acc-llamaindex/data/documents\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:36.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mFound 4 documents to process\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:36.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mLoading document: langchain_intro.txt\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:36.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mLoading document: US Trade with sub_Saharan Africa 11162023_0.pdf\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:36.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mLoading document: IncoDocs-Global-Trade-Toolkit225.pdf\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:37.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mLoading document: incoterms-chart.pdf\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:37.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mCreated 120 text chunks from 4 documents\u001b[0m\n",
      "\u001b[32m2025-10-18 19:00:39.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m123\u001b[0m - \u001b[1mSuccessfully added 120 chunks to vector store\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True\n",
      "Documents processed: 4\n",
      "Documents failed: 0\n",
      "Message: Successfully ingested 4 documents (120 chunks)\n",
      "\n",
      "Collection stats: {'collection_name': 'documents', 'document_count': 120, 'status': 'active'}\n"
     ]
    }
   ],
   "source": [
    "# Ingest documents from default directory\n",
    "result = ingest_service.ingest_documents_from_directory()\n",
    "\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Documents processed: {result.documents_processed}\")\n",
    "print(f\"Documents failed: {result.documents_failed}\")\n",
    "print(f\"Message: {result.message}\")\n",
    "print(f\"\\nCollection stats: {result.collection_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Ingest from Custom Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 18:53:16.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mStarting document ingestion from: /var/folders/zq/63gj0t315wl3ltbkp5_26d9m0000gn/T/tmp_0ynr631\u001b[0m\n",
      "\u001b[32m2025-10-18 18:53:16.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mFound 1 documents to process\u001b[0m\n",
      "\u001b[32m2025-10-18 18:53:16.637\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mLoading document: test.txt\u001b[0m\n",
      "\u001b[32m2025-10-18 18:53:16.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mCreated 1 text chunks from 1 documents\u001b[0m\n",
      "\u001b[32m2025-10-18 18:53:17.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m123\u001b[0m - \u001b[1mSuccessfully added 1 chunks to vector store\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 1 documents from custom directory\n"
     ]
    }
   ],
   "source": [
    "# Create custom directory with documents\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Create test documents\n",
    "test_doc = os.path.join(temp_dir, \"test.txt\")\n",
    "with open(test_doc, \"w\") as f:\n",
    "    f.write(\"This is a test document about artificial intelligence and machine learning.\")\n",
    "\n",
    "# Ingest from custom directory\n",
    "result = ingest_service.ingest_documents_from_directory(temp_dir)\n",
    "print(f\"Ingested {result.documents_processed} documents from custom directory\")\n",
    "\n",
    "# Cleanup\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Ingest Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 19:01:47.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_single_file\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mIngesting single file: ../data/documents/txt/langchain_intro.txt\u001b[0m\n",
      "\u001b[32m2025-10-18 19:01:47.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_single_file\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mCreated 1 text chunks from langchain_intro.txt\u001b[0m\n",
      "\u001b[32m2025-10-18 19:01:47.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_single_file\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mSuccessfully indexed file: langchain_intro.txt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True\n",
      "Message: Successfully ingested langchain_intro.txt (1 chunks)\n"
     ]
    }
   ],
   "source": [
    "# Ingest a single file\n",
    "single_file = \"../data/documents/txt/langchain_intro.txt\"\n",
    "result = ingest_service.ingest_single_file(single_file)\n",
    "\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Message: {result.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG Chat with Agents\n",
    "\n",
    "### Pattern 1: Basic Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 19:01:55.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36mreset_llm\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mLLM instance reset\u001b[0m\n",
      "\u001b[32m2025-10-18 19:01:55.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36m_initialize_llm\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mInitializing LLM with provider: openai\u001b[0m\n",
      "\u001b[32m2025-10-18 19:01:55.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mInitializing ChatOpenAI with model: gpt-5-nano-2025-08-07\u001b[0m\n",
      "\u001b[32m2025-10-18 19:01:55.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mChatOpenAI initialized successfully\u001b[0m\n",
      "\u001b[32m2025-10-18 19:01:55.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mInitializing ChatService...\u001b[0m\n",
      "\u001b[32m2025-10-18 19:01:55.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mInitializing reranking service...\u001b[0m\n",
      "\u001b[32m2025-10-18 19:01:55.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.reranking_service.service\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mInitializing CrossEncoderReranker with model: cross-encoder/ms-marco-MiniLM-L-6-v2\u001b[0m\n",
      "\u001b[32m2025-10-18 19:01:55.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.reranking_service.rerankers.cross_encoder_reranker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mLoading CrossEncoder model: cross-encoder/ms-marco-MiniLM-L-6-v2\u001b[0m\n",
      "\u001b[32m2025-10-18 19:02:01.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.reranking_service.rerankers.cross_encoder_reranker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mCrossEncoder loaded successfully\u001b[0m\n",
      "\u001b[32m2025-10-18 19:02:01.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.reranking_service.service\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mReranker initialized: cross_encoder_ms-marco-MiniLM-L-6-v2\u001b[0m\n",
      "\u001b[32m2025-10-18 19:02:01.717\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1mChatService initialized successfully\u001b[0m\n",
      "\u001b[32m2025-10-18 19:02:01.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mchat\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mProcessing chat message: What is AGOA?...\u001b[0m\n",
      "\u001b[32m2025-10-18 19:02:03.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mretrieve_context\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mReranking 20 documents to top 5\u001b[0m\n",
      "\u001b[32m2025-10-18 19:02:05.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.reranking_service.rerankers.cross_encoder_reranker\u001b[0m:\u001b[36mrerank\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mReranked 20 → 5 docs\u001b[0m\n",
      "\u001b[32m2025-10-18 19:02:11.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mretrieve_context\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mReranking 20 documents to top 5\u001b[0m\n",
      "\u001b[32m2025-10-18 19:02:11.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.reranking_service.rerankers.cross_encoder_reranker\u001b[0m:\u001b[36mrerank\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mReranked 20 → 5 docs\u001b[0m\n",
      "\u001b[32m2025-10-18 19:02:22.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mchat\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mChat response generated successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True\n",
      "\n",
      "Response:\n",
      "AGOA stands for the African Growth and Opportunity Act. It is a U.S. trade program that provides preferential access (tariff preferences) to the U.S. market for eligible sub-Saharan African countries in order to promote growth and economic opportunity.\n",
      "\n",
      "Notes from the retrieved documents:\n",
      "- The documents reference AGOA as a category of imports (AGOA imports, including oil and non-oil components) from eligible sub-Saharan African countries, and they explain that AGOA imports come from countries eligible for AGOA benefits (the data lines show AGOA imports over time). Source: Document 2; also noted in Document 1.\n"
     ]
    }
   ],
   "source": [
    "# Switch to Anthropic\n",
    "config.llm_provider = \"openai\"\n",
    "reset_llm() \n",
    "llm = get_llm()  # Now using Claude\n",
    " \n",
    "# Initialize chat service\n",
    "chat_service.initialize() \n",
    " \n",
    "# Simple chat query\n",
    "response = chat_service.chat(\"What is AGOA?\")\n",
    "\n",
    "print(f\"Success: {response['success']}\")\n",
    "print(f\"\\nResponse:\\n{response['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Chat with Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 19:06:28.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mchat\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mProcessing chat message: What are the key features of Incoterms?...\u001b[0m\n",
      "\u001b[32m2025-10-18 19:06:30.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mretrieve_context\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mReranking 20 documents to top 5\u001b[0m\n",
      "\u001b[32m2025-10-18 19:06:30.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.reranking_service.rerankers.cross_encoder_reranker\u001b[0m:\u001b[36mrerank\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mReranked 20 → 5 docs\u001b[0m\n",
      "\u001b[32m2025-10-18 19:06:40.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mchat\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mChat response generated successfully\u001b[0m\n",
      "\u001b[32m2025-10-18 19:06:40.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mchat\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mProcessing chat message: Can you explain what the docs say about software tools?...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What are the key features of Incoterms?\n",
      "Assistant: Here are the key features of Incoterms:\n",
      "\n",
      "- They are selling terms that specify who does what, who pays for what, and where the risk transfers between seller and buyer. In short, they define tasks, cos...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 19:06:43.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mretrieve_context\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mReranking 20 documents to top 5\u001b[0m\n",
      "\u001b[32m2025-10-18 19:06:43.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.reranking_service.rerankers.cross_encoder_reranker\u001b[0m:\u001b[36mrerank\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mReranked 20 → 5 docs\u001b[0m\n",
      "\u001b[32m2025-10-18 19:06:54.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mchat\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mChat response generated successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Can you explain more about Incoterms?\n",
      "Assistant: Here’s what the docs say about software/tools for importers/exporters and supply chains:\n",
      "\n",
      "- What kinds of tools exist and what they cover\n",
      "  - The docs list a range of software for global trade tasks, ...\n"
     ]
    }
   ],
   "source": [
    "# Start a conversation\n",
    "conversation_history = []\n",
    "\n",
    "# First message\n",
    "response1 = chat_service.chat(\n",
    "    \"What are the key features of Incoterms?\",\n",
    "    conversation_history=conversation_history\n",
    ")\n",
    "print(\"User: What are the key features of Incoterms?\")\n",
    "print(f\"Assistant: {response1['response'][:200]}...\\n\")\n",
    "\n",
    "# Add to history\n",
    "conversation_history.extend([\n",
    "    {\"role\": \"user\", \"content\": \"What are the key features of Incoterms?\"},\n",
    "    {\"role\": \"assistant\", \"content\": response1['response']}\n",
    "])\n",
    "\n",
    "# Follow-up message\n",
    "response2 = chat_service.chat(\n",
    "    \"Can you explain what the docs say about software tools?\",\n",
    "    conversation_history=conversation_history\n",
    ")\n",
    "print(\"User: Can you explain more about Incoterms?\")\n",
    "print(f\"Assistant: {response2['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Agent Patterns\n",
    "\n",
    "### Pattern 1: Direct Agent Creation with Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 18:54:14.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36mreset_llm\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mLLM instance reset\u001b[0m\n",
      "\u001b[32m2025-10-18 18:54:14.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36m_initialize_llm\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mInitializing LLM with provider: openai\u001b[0m\n",
      "\u001b[32m2025-10-18 18:54:14.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mInitializing ChatOpenAI with model: gpt-5-nano-2025-08-07\u001b[0m\n",
      "\u001b[32m2025-10-18 18:54:14.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mChatOpenAI initialized successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent response:\n",
      "HumanMessage: What time is it and what do the documents say about African free trade?...\n",
      "ToolMessage: 2025-10-18 18:54:17...\n",
      "ToolMessage: Subcommittee strongly prefers \n",
      "electronic submissions made through \n",
      "the Federal eRulemaking Portal: https:// \n",
      "www.regulations.gov (Regulations.gov). \n",
      "Follow the instructions for submitting \n",
      "written co...\n",
      "AIMessage: Current time: 2025-10-18 18:54:17\n",
      "\n",
      "What the documents say about African free trade (based on the retrieved content):\n",
      "- They concern AGOA — the African Growth and Opportunity Act — which is Title I of ...\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.tools import tool\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Create custom tools\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current time in a human-readable format.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "@tool\n",
    "def search_documents(query: str) -> str:\n",
    "    \"\"\"Search the document knowledge base for relevant information.\"\"\"\n",
    "    vector_store = chroma_client.get_vector_store()\n",
    "    docs = vector_store.similarity_search(query, k=3)\n",
    "    if not docs:\n",
    "        return \"No relevant documents found.\"\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Create agent with multiple tools\n",
    "config.llm_provider = \"openai\"\n",
    "reset_llm() \n",
    "llm = get_llm()\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[get_current_time, search_documents],\n",
    "    system_prompt=\"You are a helpful assistant with access to document search and time utilities.\"\n",
    ")\n",
    " \n",
    "# Test the agent\n",
    "response = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What time is it and what do the documents say about African free trade?\"}]\n",
    "})\n",
    "\n",
    "print(\"Agent response:\")\n",
    "for msg in response[\"messages\"]:\n",
    "    if hasattr(msg, 'content') and msg.content:\n",
    "        print(f\"{msg.__class__.__name__}: {msg.content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Agent with Dynamic Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using basic model (message count: 1)\n",
      "\n",
      "Simple query completed\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.middleware import wrap_model_call, ModelRequest\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Create different models for different complexity\n",
    "basic_model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0.3)\n",
    "advanced_model = ChatOpenAI(model=\"gpt-5-mini-2025-08-07\", temperature=0.7)\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler):\n",
    "    \"\"\"Select model based on message count.\"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "    \n",
    "    # Use advanced model for complex conversations\n",
    "    if message_count > 5:\n",
    "        print(f\"Using advanced model (message count: {message_count})\")\n",
    "        request.model = advanced_model\n",
    "    else:\n",
    "        print(f\"Using basic model (message count: {message_count})\")\n",
    "        request.model = basic_model\n",
    "    \n",
    "    return handler(request)\n",
    "\n",
    "# Create agent with dynamic model selection\n",
    "agent = create_agent(\n",
    "    model=basic_model,\n",
    "    tools=[search_documents],\n",
    "    middleware=[dynamic_model_selection]\n",
    ")\n",
    "\n",
    "# Test with simple query\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]})\n",
    "print(\"\\nSimple query completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retriever Customization\n",
    "\n",
    "### Pattern 1: Custom Retriever with Score Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are all the incoterms in the Incoterms 2022?\n",
      "\n",
      "Result 1 (score: 0.6255):\n",
      "to, which may occur when it is being used to confirm complex commercial agreements.\n",
      "All parties must make it clear in contracts which Incoterms® version is being referred to in order to avoid any\n",
      "misunderstanding. Different trading partners will incorporate Incoterms® into contracts at different times.\n",
      "It is imperative that you check existing contr...\n",
      "\n",
      "Result 2 (score: 0.6255):\n",
      "to, which may occur when it is being used to confirm complex commercial agreements.\n",
      "All parties must make it clear in contracts which Incoterms® version is being referred to in order to avoid any\n",
      "misunderstanding. Different trading partners will incorporate Incoterms® into contracts at different times.\n",
      "It is imperative that you check existing contr...\n",
      "\n",
      "Result 3 (score: 0.6255):\n",
      "to, which may occur when it is being used to confirm complex commercial agreements.\n",
      "All parties must make it clear in contracts which Incoterms® version is being referred to in order to avoid any\n",
      "misunderstanding. Different trading partners will incorporate Incoterms® into contracts at different times.\n",
      "It is imperative that you check existing contr...\n",
      "\n",
      "Result 4 (score: 0.6255):\n",
      "to, which may occur when it is being used to confirm complex commercial agreements.\n",
      "All parties must make it clear in contracts which Incoterms® version is being referred to in order to avoid any\n",
      "misunderstanding. Different trading partners will incorporate Incoterms® into contracts at different times.\n",
      "It is imperative that you check existing contr...\n",
      "\n",
      "Result 5 (score: 0.6255):\n",
      "to, which may occur when it is being used to confirm complex commercial agreements.\n",
      "All parties must make it clear in contracts which Incoterms® version is being referred to in order to avoid any\n",
      "misunderstanding. Different trading partners will incorporate Incoterms® into contracts at different times.\n",
      "It is imperative that you check existing contr...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get vector store\n",
    "vector_store = chroma_client.get_vector_store() \n",
    "\n",
    "# Similarity search with scores\n",
    "query = \"What are all the incoterms in the Incoterms 2022?\"\n",
    "results = vector_store.similarity_search_with_score(query, k=5)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"Result {i} (score: {score:.4f}):\")\n",
    "    print(f\"{doc.page_content[:350]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Multi-Query Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1 unique documents from 3 queries\n",
      "\n",
      "Document 1:\n",
      "Put simply, Incoterms® are the selling terms that the buyer and seller of goods both agrees to.  The Incoterm®\n",
      "clearly states which tasks, costs and r...\n"
     ]
    }
   ],
   "source": [
    "# Multiple related queries\n",
    "queries = [\n",
    "    \"What are incoterms?\",\n",
    "    \"How do incoterms affect shipping?\",\n",
    "    \"What are the benefits of incoterms?\"\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "for query in queries:\n",
    "    results = vector_store.similarity_search(query, k=2)\n",
    "    all_results.extend(results)\n",
    "\n",
    "# Deduplicate based on content\n",
    "unique_docs = {}\n",
    "for doc in all_results:\n",
    "    unique_docs[doc.page_content[:100]] = doc\n",
    "\n",
    "print(f\"Retrieved {len(unique_docs)} unique documents from {len(queries)} queries\")\n",
    "for i, doc in enumerate(list(unique_docs.values())[:3], 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"{doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conversation Memory\n",
    "\n",
    "### Pattern 1: Persistent Conversation with Checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 18:54:26.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36mreset_llm\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mLLM instance reset\u001b[0m\n",
      "\u001b[32m2025-10-18 18:54:26.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36m_initialize_llm\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mInitializing LLM with provider: groq\u001b[0m\n",
      "\u001b[32m2025-10-18 18:54:26.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.groq_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mInitializing ChatGroq with model: openai/gpt-oss-120b\u001b[0m\n",
      "\u001b[32m2025-10-18 18:54:26.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.groq_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mChatGroq initialized successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: My name is Alice\n",
      "Assistant: Nice to meet you, Alice!...\n",
      "\n",
      "User: What's my name?\n",
      "Assistant: Your name is Alice.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from acc_llamaindex.config import config\n",
    "\n",
    "# Create agent with memory\n",
    "checkpointer = InMemorySaver() \n",
    "\n",
    "#toggle between providers\n",
    "config.llm_provider = \"groq\"\n",
    "reset_llm() \n",
    "llm = get_llm() \n",
    "\n",
    "agent = create_agent(\n",
    "    model=get_llm(),\n",
    "    tools=[search_documents],\n",
    "    checkpointer=checkpointer,\n",
    "    system_prompt=\"You are a helpful assistant. Remember the conversation context and limit answers to 5 words.\"\n",
    ")\n",
    "\n",
    "# Conversation thread\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"user-123\"}}\n",
    "\n",
    "# First message\n",
    "response1 = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is Alice\"}]},\n",
    "    config\n",
    ")\n",
    "print(\"User: My name is Alice\")\n",
    "print(f\"Assistant: {[m for m in response1['messages'] if hasattr(m, 'content')][-1].content[:100]}...\\n\")\n",
    "\n",
    "# Second message (should remember name)\n",
    "response2 = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n",
    "    config\n",
    ")\n",
    "print(\"User: What's my name?\")\n",
    "print(f\"Assistant: {[m for m in response2['messages'] if hasattr(m, 'content')][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Structured Outputs\n",
    "\n",
    "### Pattern 1: Extract Structured Data from Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'llm_provider'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     category: \u001b[38;5;28mstr\u001b[39m = Field(description=\u001b[33m\"\u001b[39m\u001b[33mDocument category (e.g., technical, guide, reference)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# reset llm\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllm_provider\u001b[49m=\u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m reset_llm()\n\u001b[32m     15\u001b[39m llm = get_llm()\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'llm_provider'"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "\n",
    "# Define schema\n",
    "class DocumentSummary(BaseModel):\n",
    "    \"\"\"Summary of document content.\"\"\"\n",
    "    title: str = Field(description=\"Main topic or title\")\n",
    "    key_points: list[str] = Field(description=\"List of key points mentioned\")\n",
    "    category: str = Field(description=\"Document category (e.g., technical, guide, reference)\")\n",
    "\n",
    "# reset llm\n",
    "config.llm_provider=\"openai\"\n",
    "reset_llm()\n",
    "llm = get_llm()\n",
    "\n",
    "# Create a structured output LLM (without agent)\n",
    "structured_llm = llm.with_structured_output(DocumentSummary)\n",
    "\n",
    "# First, search for relevant documents\n",
    "vector_store = chroma_client.get_vector_store()\n",
    "docs = vector_store.similarity_search(\"Incoterms\", k=5)\n",
    "context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
    "\n",
    "# Then use structured output to summarize\n",
    "prompt = f\"\"\"Based on the following documents, provide a structured summary about Incoterms:\n",
    "\n",
    "{context}\n",
    "\n",
    "Provide a title, category, and key points.\"\"\"\n",
    "\n",
    "summary = structured_llm.invoke(prompt)\n",
    "\n",
    "print(f\"Title: {summary.title}\")\n",
    "print(f\"Category: {summary.category}\")\n",
    "print(f\"\\nKey Points:\")\n",
    "for i, point in enumerate(summary.key_points, 1):\n",
    "    print(f\"{i}. {point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Handling and Observability\n",
    "\n",
    "### Pattern 1: Graceful Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent handled potential errors gracefully\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain_core.messages import ToolMessage \n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"Catch and handle tool execution errors.\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        print(f\"Tool error caught: {str(e)[:100]}\")\n",
    "        return ToolMessage(\n",
    "            content=f\"Tool execution failed. Please try rephrasing your request.\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "# Create agent with error handling\n",
    "agent = create_agent(\n",
    "    model=get_llm(),\n",
    "    tools=[search_documents],\n",
    "    middleware=[handle_tool_errors]\n",
    ")\n",
    "\n",
    "# Test with query\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Search for information\"}]})\n",
    "print(\"Agent handled potential errors gracefully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: LangSmith Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langsmith import traceable\n",
    "\n",
    "# Check if LangSmith is configured\n",
    "if os.getenv(\"LANGCHAIN_API_KEY\"):\n",
    "    print(\"LangSmith tracing is enabled\")\n",
    "    print(f\"Project: {os.getenv('LANGCHAIN_PROJECT', 'default')}\")\n",
    "    \n",
    "    @traceable(run_type=\"chain\", name=\"custom_rag_chain\")\n",
    "    def custom_rag_chain(query: str) -> str:\n",
    "        \"\"\"Custom RAG chain with tracing.\"\"\"\n",
    "        # Retrieve documents\n",
    "        vector_store = chroma_client.get_vector_store()\n",
    "        docs = vector_store.similarity_search(query, k=3)\n",
    "        \n",
    "        # Generate response\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        llm = get_llm()\n",
    "        response = llm.invoke(\n",
    "            f\"Based on this context:\\n{context}\\n\\nAnswer: {query}\"\n",
    "        )\n",
    "        return response.content\n",
    "    \n",
    "    # Test with tracing\n",
    "    result = custom_rag_chain(\"What are the benefits of RAG?\")\n",
    "    print(f\"\\nResponse: {result[:200]}...\")\n",
    "    print(\"\\n✓ Check LangSmith for full trace details\")\n",
    "else:\n",
    "    print(\"LangSmith not configured. Set LANGCHAIN_API_KEY to enable tracing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Document Ingestion**: Multiple patterns for loading documents into the vector store\n",
    "2. **RAG Chat**: Basic and conversational chat patterns\n",
    "3. **Advanced Agents**: Custom tools, dynamic model selection, and middleware\n",
    "4. **Retrievers**: Custom retrieval strategies and multi-query patterns\n",
    "5. **Memory**: Conversation persistence with checkpointers\n",
    "6. **Structured Outputs**: Extracting validated data from LLM responses\n",
    "7. **Error Handling**: Graceful degradation and tool error management\n",
    "8. **Observability**: LangSmith tracing integration\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Experiment with different embedding models\n",
    "- Try various chunk sizes and overlaps\n",
    "- Implement reranking for better retrieval\n",
    "- Add evaluation metrics (faithfulness, relevance, etc.)\n",
    "- Explore LangGraph for complex workflows\n",
    "- Add streaming responses for better UX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
