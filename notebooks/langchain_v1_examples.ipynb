{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain v1 RAG Application Examples\n",
    "\n",
    "This notebook demonstrates usage patterns for the refactored RAG application using **LangChain v1**.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Initialization\n",
    "2. Document Ingestion Patterns\n",
    "3. RAG Chat with Agents\n",
    "4. Advanced Agent Patterns\n",
    "5. Retriever Customization\n",
    "6. Conversation Memory\n",
    "7. Structured Outputs\n",
    "8. Error Handling and Observability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-17 20:04:28.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mIngestDocumentsService initialized with documents_path: /Users/kevinknox/coding/acc-llamaindex/data/documents\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:28.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mChatService initialized\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Add project root to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Verify imports\n",
    "from acc_llamaindex.config import config\n",
    "from acc_llamaindex.infrastructure.llm_providers.langchain_provider import get_llm, get_embeddings, reset_llm\n",
    "from acc_llamaindex.infrastructure.db.chroma_client import chroma_client\n",
    "from acc_llamaindex.application.ingest_documents_service.service import ingest_service\n",
    "from acc_llamaindex.application.chat_service.service import chat_service\n",
    "from acc_llamaindex.infrastructure.llm_providers import langchain_provider, anthropic_provider \n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Ingestion Patterns\n",
    "\n",
    "### Pattern 1: Basic Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-17 20:04:30.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36m_initialize_llm\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mInitializing LLM with provider: openai\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:30.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mInitializing ChatOpenAI with model: gpt-5-nano-2025-08-07\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:31.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mChatOpenAI initialized successfully\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:31.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36m_initialize_embeddings\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mInitializing embeddings with provider: openai\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:31.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_embeddings\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mInitializing OpenAIEmbeddings with model: text-embedding-3-small\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:31.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_embeddings\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mOpenAIEmbeddings initialized successfully\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:31.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.db.chroma_client\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mInitializing ChromaDB at /Users/kevinknox/coding/acc-llamaindex/data/chroma_db\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:31.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.db.chroma_client\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mChromaDB initialized successfully with collection: documents\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents path: /Users/kevinknox/coding/acc-llamaindex/data/documents\n",
      "ChromaDB path: /Users/kevinknox/coding/acc-llamaindex/data/chroma_db\n",
      "Chunk size: 1024, overlap: 200\n"
     ]
    }
   ],
   "source": [
    "# Initialize services\n",
    "get_llm()\n",
    "get_embeddings()\n",
    "chroma_client.initialize()\n",
    "\n",
    "print(f\"Documents path: {config.documents_path}\")\n",
    "print(f\"ChromaDB path: {config.chroma_db_path}\")\n",
    "print(f\"Chunk size: {config.chunk_size}, overlap: {config.chunk_overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-17 20:04:33.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mStarting document ingestion from: /Users/kevinknox/coding/acc-llamaindex/data/documents\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:33.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mFound 4 documents to process\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:33.451\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mLoading document: langchain_intro.txt\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:33.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mLoading document: US Trade with sub_Saharan Africa 11162023_0.pdf\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:33.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mLoading document: IncoDocs-Global-Trade-Toolkit225.pdf\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:34.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mLoading document: incoterms-chart.pdf\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:34.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mCreated 120 text chunks from 4 documents\u001b[0m\n",
      "\u001b[32m2025-10-17 20:04:35.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m123\u001b[0m - \u001b[1mSuccessfully added 120 chunks to vector store\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True\n",
      "Documents processed: 4\n",
      "Documents failed: 0\n",
      "Message: Successfully ingested 4 documents (120 chunks)\n",
      "\n",
      "Collection stats: {'collection_name': 'documents', 'document_count': 20924, 'status': 'active'}\n"
     ]
    }
   ],
   "source": [
    "# Ingest documents from default directory\n",
    "result = ingest_service.ingest_documents_from_directory()\n",
    "\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Documents processed: {result.documents_processed}\")\n",
    "print(f\"Documents failed: {result.documents_failed}\")\n",
    "print(f\"Message: {result.message}\")\n",
    "print(f\"\\nCollection stats: {result.collection_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Ingest from Custom Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-17 20:02:11.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mStarting document ingestion from: /var/folders/zq/63gj0t315wl3ltbkp5_26d9m0000gn/T/tmpq2rxy3ed\u001b[0m\n",
      "\u001b[32m2025-10-17 20:02:11.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mFound 1 documents to process\u001b[0m\n",
      "\u001b[32m2025-10-17 20:02:11.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mLoading document: test.txt\u001b[0m\n",
      "\u001b[32m2025-10-17 20:02:11.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m117\u001b[0m - \u001b[1mCreated 1 text chunks from 1 documents\u001b[0m\n",
      "\u001b[32m2025-10-17 20:02:12.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_documents_from_directory\u001b[0m:\u001b[36m123\u001b[0m - \u001b[1mSuccessfully added 1 chunks to vector store\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 1 documents from custom directory\n"
     ]
    }
   ],
   "source": [
    "# Create custom directory with documents\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Create test documents\n",
    "test_doc = os.path.join(temp_dir, \"test.txt\")\n",
    "with open(test_doc, \"w\") as f:\n",
    "    f.write(\"This is a test document about artificial intelligence and machine learning.\")\n",
    "\n",
    "# Ingest from custom directory\n",
    "result = ingest_service.ingest_documents_from_directory(temp_dir)\n",
    "print(f\"Ingested {result.documents_processed} documents from custom directory\")\n",
    "\n",
    "# Cleanup\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Ingest Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-17 20:05:05.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_single_file\u001b[0m:\u001b[36m158\u001b[0m - \u001b[1mIngesting single file: ../data/documents/txt/langchain_intro.txt\u001b[0m\n",
      "\u001b[32m2025-10-17 20:05:05.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_single_file\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mCreated 1 text chunks from langchain_intro.txt\u001b[0m\n",
      "\u001b[32m2025-10-17 20:05:13.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.ingest_documents_service.service\u001b[0m:\u001b[36mingest_single_file\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mSuccessfully indexed file: langchain_intro.txt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True\n",
      "Message: Successfully ingested langchain_intro.txt (1 chunks)\n"
     ]
    }
   ],
   "source": [
    "# Ingest a single file\n",
    "single_file = \"../data/documents/txt/langchain_intro.txt\"\n",
    "result = ingest_service.ingest_single_file(single_file)\n",
    "\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Message: {result.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG Chat with Agents\n",
    "\n",
    "### Pattern 1: Basic Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-17 20:05:29.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36mreset_llm\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mLLM instance reset\u001b[0m\n",
      "\u001b[32m2025-10-17 20:05:29.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36m_initialize_llm\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mInitializing LLM with provider: openai\u001b[0m\n",
      "\u001b[32m2025-10-17 20:05:29.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mInitializing ChatOpenAI with model: gpt-5-nano-2025-08-07\u001b[0m\n",
      "\u001b[32m2025-10-17 20:05:29.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mChatOpenAI initialized successfully\u001b[0m\n",
      "\u001b[32m2025-10-17 20:05:29.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mInitializing ChatService...\u001b[0m\n",
      "\u001b[32m2025-10-17 20:05:29.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mChatService initialized successfully\u001b[0m\n",
      "\u001b[32m2025-10-17 20:05:29.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mchat\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mProcessing chat message: What is AGOA?...\u001b[0m\n",
      "\u001b[32m2025-10-17 20:05:48.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mchat\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mChat response generated successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True\n",
      "\n",
      "Response:\n",
      "Short answer: AGOA stands for the Africa Growth and Opportunity Act. It is a U.S. trade act enacted in 2000 to provide enhanced access to the U.S. market for eligible sub-Saharan African countries, including duty-free or preferential treatment for many products and other trade benefits intended to support economic development.\n",
      "\n",
      "What I can verify from the knowledge base you provided:\n",
      "- The documents reference AGOA in connection with section 502 of the 1974 Act and note that the President must monitor and annually review progress under section 506A. They do not provide a full definition of AGOA within those excerpts.\n",
      "\n",
      "If you’d like, I can fetch more sources that give a fuller definition and details (eligibility criteria, product coverage, renewal terms, etc.). Sources cited: Documents mentioning AGOA (relating to the 1974 Act) in your knowledge base.\n"
     ]
    }
   ],
   "source": [
    "# Switch to Anthropic\n",
    "config.llm_provider = \"openai\"\n",
    "reset_llm() \n",
    "llm = get_llm()  # Now using Claude\n",
    " \n",
    "# Initialize chat service\n",
    "chat_service.initialize() \n",
    " \n",
    "# Simple chat query\n",
    "response = chat_service.chat(\"What is AGOA?\")\n",
    "\n",
    "print(f\"Success: {response['success']}\")\n",
    "print(f\"\\nResponse:\\n{response['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Chat with Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-17 20:06:53.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mchat\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mProcessing chat message: What are the key features of Incoterms?...\u001b[0m\n",
      "\u001b[32m2025-10-17 20:07:30.337\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mchat\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mChat response generated successfully\u001b[0m\n",
      "\u001b[32m2025-10-17 20:07:30.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mchat\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mProcessing chat message: Can you explain more about Incoterms?...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What are the key features of Incoterms?\n",
      "Assistant: Here’s a concise overview of the key features of Incoterms (Incoterms 2020), as described in the Incoterms 2020 Rules Responsibility Quick Reference Guide in the knowledge base:\n",
      "\n",
      "- Allocation of respo...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-17 20:07:54.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.application.chat_service.service\u001b[0m:\u001b[36mchat\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mChat response generated successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Can you explain more about Incoterms?\n",
      "Assistant: Here’s a more in-depth, but still high-level, explanation of Incoterms and how they work.\n",
      "\n",
      "What Incoterms are\n",
      "- Incoterms are a set of standardized trade terms published by the International Chamber o...\n"
     ]
    }
   ],
   "source": [
    "# Start a conversation\n",
    "conversation_history = []\n",
    "\n",
    "# First message\n",
    "response1 = chat_service.chat(\n",
    "    \"What are the key features of Incoterms?\",\n",
    "    conversation_history=conversation_history\n",
    ")\n",
    "print(\"User: What are the key features of Incoterms?\")\n",
    "print(f\"Assistant: {response1['response'][:200]}...\\n\")\n",
    "\n",
    "# Add to history\n",
    "conversation_history.extend([\n",
    "    {\"role\": \"user\", \"content\": \"What are the key features of Incoterms?\"},\n",
    "    {\"role\": \"assistant\", \"content\": response1['response']}\n",
    "])\n",
    "\n",
    "# Follow-up message\n",
    "response2 = chat_service.chat(\n",
    "    \"Can you explain more about Incoterms?\",\n",
    "    conversation_history=conversation_history\n",
    ")\n",
    "print(\"User: Can you explain more about Incoterms?\")\n",
    "print(f\"Assistant: {response2['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Agent Patterns\n",
    "\n",
    "### Pattern 1: Direct Agent Creation with Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-17 12:16:06.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36mreset_llm\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mLLM instance reset\u001b[0m\n",
      "\u001b[32m2025-10-17 12:16:06.308\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36m_initialize_llm\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mInitializing LLM with provider: openai\u001b[0m\n",
      "\u001b[32m2025-10-17 12:16:06.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mInitializing ChatOpenAI with model: gpt-5-nano-2025-08-07\u001b[0m\n",
      "\u001b[32m2025-10-17 12:16:06.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mChatOpenAI initialized successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent response:\n",
      "HumanMessage: What time is it and what do the documents say about African free trade?...\n",
      "ToolMessage: 2025-10-17 12:16:10...\n",
      "ToolMessage: Subcommittee strongly prefers \n",
      "electronic submissions made through \n",
      "the Federal eRulemaking Portal: https:// \n",
      "www.regulations.gov (Regulations.gov). \n",
      "Follow the instructions for submitting \n",
      "written co...\n",
      "AIMessage: Current time: 12:16:10 on October 17, 2025.\n",
      "\n",
      "Documents found:\n",
      "- The materials returned focus on AGOA (the African Growth and Opportunity Act), not specifically on AfCFTA (the African Continental Free ...\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.tools import tool\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Create custom tools\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current time in a human-readable format.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "@tool\n",
    "def search_documents(query: str) -> str:\n",
    "    \"\"\"Search the document knowledge base for relevant information.\"\"\"\n",
    "    vector_store = chroma_client.get_vector_store()\n",
    "    docs = vector_store.similarity_search(query, k=3)\n",
    "    if not docs:\n",
    "        return \"No relevant documents found.\"\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Create agent with multiple tools\n",
    "config.llm_provider = \"openai\"\n",
    "reset_llm() \n",
    "llm = get_llm()\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[get_current_time, search_documents],\n",
    "    system_prompt=\"You are a helpful assistant with access to document search and time utilities.\"\n",
    ")\n",
    " \n",
    "# Test the agent\n",
    "response = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What time is it and what do the documents say about African free trade?\"}]\n",
    "})\n",
    "\n",
    "print(\"Agent response:\")\n",
    "for msg in response[\"messages\"]:\n",
    "    if hasattr(msg, 'content') and msg.content:\n",
    "        print(f\"{msg.__class__.__name__}: {msg.content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Agent with Dynamic Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using basic model (message count: 1)\n",
      "\n",
      "Simple query completed\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.middleware import wrap_model_call, ModelRequest\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Create different models for different complexity\n",
    "basic_model = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0.3)\n",
    "advanced_model = ChatOpenAI(model=\"gpt-5-mini-2025-08-07\", temperature=0.7)\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler):\n",
    "    \"\"\"Select model based on message count.\"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "    \n",
    "    # Use advanced model for complex conversations\n",
    "    if message_count > 5:\n",
    "        print(f\"Using advanced model (message count: {message_count})\")\n",
    "        request.model = advanced_model\n",
    "    else:\n",
    "        print(f\"Using basic model (message count: {message_count})\")\n",
    "        request.model = basic_model\n",
    "    \n",
    "    return handler(request)\n",
    "\n",
    "# Create agent with dynamic model selection\n",
    "agent = create_agent(\n",
    "    model=basic_model,\n",
    "    tools=[search_documents],\n",
    "    middleware=[dynamic_model_selection]\n",
    ")\n",
    "\n",
    "# Test with simple query\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]})\n",
    "print(\"\\nSimple query completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retriever Customization\n",
    "\n",
    "### Pattern 1: Custom Retriever with Score Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are all the incoterms in the Incoterms 2022?\n",
      "\n",
      "Result 1 (score: 0.6255):\n",
      "to, which may occur when it is being used to confirm complex commercial agreements.\n",
      "All parties must make it clear in contracts which Incoterms® version is being referred to in order to avoid any\n",
      "misunderstanding. Different trading partners will incorporate Incoterms® into contracts at different times.\n",
      "It is imperative that you check existing contr...\n",
      "\n",
      "Result 2 (score: 0.6255):\n",
      "to, which may occur when it is being used to confirm complex commercial agreements.\n",
      "All parties must make it clear in contracts which Incoterms® version is being referred to in order to avoid any\n",
      "misunderstanding. Different trading partners will incorporate Incoterms® into contracts at different times.\n",
      "It is imperative that you check existing contr...\n",
      "\n",
      "Result 3 (score: 0.6255):\n",
      "to, which may occur when it is being used to confirm complex commercial agreements.\n",
      "All parties must make it clear in contracts which Incoterms® version is being referred to in order to avoid any\n",
      "misunderstanding. Different trading partners will incorporate Incoterms® into contracts at different times.\n",
      "It is imperative that you check existing contr...\n",
      "\n",
      "Result 4 (score: 0.6255):\n",
      "to, which may occur when it is being used to confirm complex commercial agreements.\n",
      "All parties must make it clear in contracts which Incoterms® version is being referred to in order to avoid any\n",
      "misunderstanding. Different trading partners will incorporate Incoterms® into contracts at different times.\n",
      "It is imperative that you check existing contr...\n",
      "\n",
      "Result 5 (score: 0.6255):\n",
      "to, which may occur when it is being used to confirm complex commercial agreements.\n",
      "All parties must make it clear in contracts which Incoterms® version is being referred to in order to avoid any\n",
      "misunderstanding. Different trading partners will incorporate Incoterms® into contracts at different times.\n",
      "It is imperative that you check existing contr...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get vector store\n",
    "vector_store = chroma_client.get_vector_store() \n",
    "\n",
    "# Similarity search with scores\n",
    "query = \"What are all the incoterms in the Incoterms 2022?\"\n",
    "results = vector_store.similarity_search_with_score(query, k=5)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"Result {i} (score: {score:.4f}):\")\n",
    "    print(f\"{doc.page_content[:350]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Multi-Query Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1 unique documents from 3 queries\n",
      "\n",
      "Document 1:\n",
      "Put simply, Incoterms® are the selling terms that the buyer and seller of goods both agrees to.  The Incoterm®\n",
      "clearly states which tasks, costs and r...\n"
     ]
    }
   ],
   "source": [
    "# Multiple related queries\n",
    "queries = [\n",
    "    \"What are incoterms?\",\n",
    "    \"How do incoterms affect shipping?\",\n",
    "    \"What are the benefits of incoterms?\"\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "for query in queries:\n",
    "    results = vector_store.similarity_search(query, k=2)\n",
    "    all_results.extend(results)\n",
    "\n",
    "# Deduplicate based on content\n",
    "unique_docs = {}\n",
    "for doc in all_results:\n",
    "    unique_docs[doc.page_content[:100]] = doc\n",
    "\n",
    "print(f\"Retrieved {len(unique_docs)} unique documents from {len(queries)} queries\")\n",
    "for i, doc in enumerate(list(unique_docs.values())[:3], 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"{doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conversation Memory\n",
    "\n",
    "### Pattern 1: Persistent Conversation with Checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-17 18:19:32.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36mreset_llm\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mLLM instance reset\u001b[0m\n",
      "\u001b[32m2025-10-17 18:19:32.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36m_initialize_llm\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mInitializing LLM with provider: groq\u001b[0m\n",
      "\u001b[32m2025-10-17 18:19:32.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.groq_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mInitializing ChatGroq with model: openai/gpt-oss-120b\u001b[0m\n",
      "\u001b[32m2025-10-17 18:19:32.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.groq_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mChatGroq initialized successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: My name is Alice\n",
      "Assistant: Nice to meet you, Alice....\n",
      "\n",
      "User: What's my name?\n",
      "Assistant: Your name is Alice.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from acc_llamaindex.config import config\n",
    "\n",
    "# Create agent with memory\n",
    "checkpointer = InMemorySaver() \n",
    "\n",
    "#toggle between providers\n",
    "config.llm_provider = \"groq\"\n",
    "reset_llm() \n",
    "llm = get_llm() \n",
    "\n",
    "agent = create_agent(\n",
    "    model=get_llm(),\n",
    "    tools=[search_documents],\n",
    "    checkpointer=checkpointer,\n",
    "    system_prompt=\"You are a helpful assistant. Remember the conversation context and limit answers to 5 words.\"\n",
    ")\n",
    "\n",
    "# Conversation thread\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"user-123\"}}\n",
    "\n",
    "# First message\n",
    "response1 = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is Alice\"}]},\n",
    "    config\n",
    ")\n",
    "print(\"User: My name is Alice\")\n",
    "print(f\"Assistant: {[m for m in response1['messages'] if hasattr(m, 'content')][-1].content[:100]}...\\n\")\n",
    "\n",
    "# Second message (should remember name)\n",
    "response2 = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]},\n",
    "    config\n",
    ")\n",
    "print(\"User: What's my name?\")\n",
    "print(f\"Assistant: {[m for m in response2['messages'] if hasattr(m, 'content')][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Structured Outputs\n",
    "\n",
    "### Pattern 1: Extract Structured Data from Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-17 20:12:09.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36mreset_llm\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mLLM instance reset\u001b[0m\n",
      "\u001b[32m2025-10-17 20:12:09.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.langchain_provider\u001b[0m:\u001b[36m_initialize_llm\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mInitializing LLM with provider: openai\u001b[0m\n",
      "\u001b[32m2025-10-17 20:12:09.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mInitializing ChatOpenAI with model: gpt-5-nano-2025-08-07\u001b[0m\n",
      "\u001b[32m2025-10-17 20:12:09.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macc_llamaindex.infrastructure.llm_providers.openai_provider\u001b[0m:\u001b[36minitialize_llm\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mChatOpenAI initialized successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Incoterms 2020 Rules Responsibility Quick Reference Guide\n",
      "Category: Technical reference / Guide\n",
      "\n",
      "Key Points:\n",
      "1. Purpose: Incoterms define the allocation of responsibilities, costs and risks between seller and buyer for international shipments.\n",
      "2. Scope: The 2020 rules comprise 11 terms organized into four groups: E (EXW), F (FCA, FAS, FOB), C (CFR, CIF, CPT, CIP), and D (DAP, DPU, DDP).\n",
      "3. Risk transfer: The point at which risk passes from seller to buyer varies by term (e.g., EXW at seller’s premises; FOB when goods pass the ship’s rail; DAP/DPU when delivered at the named place; DDP at import or delivery).\n",
      "4. Cost allocation: Primary cost responsibilities (freight, insurance, export/import formalities, duties) differ by term (e.g., CFR/CIF: seller pays main carriage; CIF also requires minimum insurance; EXW: buyer bears most costs).\n",
      "5. Key changes in 2020: DPU replaces the former DAT term (Delivered at Place Unloaded); DAP and DDP remain; CIP and CIF specify insurance requirements; terms clarify who handles export and import clearance and duties.\n",
      "6. Practical use: Contracts should specify the exact Incoterm, named place, and whether unloading is included, to avoid disputes over risk transfer, costs, and compliance obligations.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define schema\n",
    "class DocumentSummary(BaseModel):\n",
    "    \"\"\"Summary of document content.\"\"\"\n",
    "    title: str = Field(description=\"Main topic or title\")\n",
    "    key_points: list[str] = Field(description=\"List of key points mentioned\")\n",
    "    category: str = Field(description=\"Document category (e.g., technical, guide, reference)\")\n",
    "\n",
    "# reset llm\n",
    "config.llm_provider=\"openai\"\n",
    "reset_llm()\n",
    "llm = get_llm()\n",
    "\n",
    "# Create a structured output LLM (without agent)\n",
    "structured_llm = llm.with_structured_output(DocumentSummary)\n",
    "\n",
    "# First, search for relevant documents\n",
    "vector_store = chroma_client.get_vector_store()\n",
    "docs = vector_store.similarity_search(\"Incoterms\", k=5)\n",
    "context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
    "\n",
    "# Then use structured output to summarize\n",
    "prompt = f\"\"\"Based on the following documents, provide a structured summary about Incoterms:\n",
    "\n",
    "{context}\n",
    "\n",
    "Provide a title, category, and key points.\"\"\"\n",
    "\n",
    "summary = structured_llm.invoke(prompt)\n",
    "\n",
    "print(f\"Title: {summary.title}\")\n",
    "print(f\"Category: {summary.category}\")\n",
    "print(f\"\\nKey Points:\")\n",
    "for i, point in enumerate(summary.key_points, 1):\n",
    "    print(f\"{i}. {point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Handling and Observability\n",
    "\n",
    "### Pattern 1: Graceful Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"Catch and handle tool execution errors.\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        print(f\"Tool error caught: {str(e)[:100]}\")\n",
    "        return ToolMessage(\n",
    "            content=f\"Tool execution failed. Please try rephrasing your request.\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "# Create agent with error handling\n",
    "agent = create_agent(\n",
    "    model=get_llm(),\n",
    "    tools=[search_documents],\n",
    "    middleware=[handle_tool_errors]\n",
    ")\n",
    "\n",
    "# Test with query\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Search for information\"}]})\n",
    "print(\"Agent handled potential errors gracefully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: LangSmith Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langsmith import traceable\n",
    "\n",
    "# Check if LangSmith is configured\n",
    "if os.getenv(\"LANGCHAIN_API_KEY\"):\n",
    "    print(\"LangSmith tracing is enabled\")\n",
    "    print(f\"Project: {os.getenv('LANGCHAIN_PROJECT', 'default')}\")\n",
    "    \n",
    "    @traceable(run_type=\"chain\", name=\"custom_rag_chain\")\n",
    "    def custom_rag_chain(query: str) -> str:\n",
    "        \"\"\"Custom RAG chain with tracing.\"\"\"\n",
    "        # Retrieve documents\n",
    "        vector_store = chroma_client.get_vector_store()\n",
    "        docs = vector_store.similarity_search(query, k=3)\n",
    "        \n",
    "        # Generate response\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        llm = get_llm()\n",
    "        response = llm.invoke(\n",
    "            f\"Based on this context:\\n{context}\\n\\nAnswer: {query}\"\n",
    "        )\n",
    "        return response.content\n",
    "    \n",
    "    # Test with tracing\n",
    "    result = custom_rag_chain(\"What are the benefits of RAG?\")\n",
    "    print(f\"\\nResponse: {result[:200]}...\")\n",
    "    print(\"\\n✓ Check LangSmith for full trace details\")\n",
    "else:\n",
    "    print(\"LangSmith not configured. Set LANGCHAIN_API_KEY to enable tracing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Document Ingestion**: Multiple patterns for loading documents into the vector store\n",
    "2. **RAG Chat**: Basic and conversational chat patterns\n",
    "3. **Advanced Agents**: Custom tools, dynamic model selection, and middleware\n",
    "4. **Retrievers**: Custom retrieval strategies and multi-query patterns\n",
    "5. **Memory**: Conversation persistence with checkpointers\n",
    "6. **Structured Outputs**: Extracting validated data from LLM responses\n",
    "7. **Error Handling**: Graceful degradation and tool error management\n",
    "8. **Observability**: LangSmith tracing integration\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Experiment with different embedding models\n",
    "- Try various chunk sizes and overlaps\n",
    "- Implement reranking for better retrieval\n",
    "- Add evaluation metrics (faithfulness, relevance, etc.)\n",
    "- Explore LangGraph for complex workflows\n",
    "- Add streaming responses for better UX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
