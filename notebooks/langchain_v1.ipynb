{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b54090b",
   "metadata": {},
   "source": [
    "### Static Model\n",
    "Static models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach. To initialize a static model from a model identifier string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca9ffe10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='40b2fe6c-4120-49d2-8b51-0204c44853a0'),\n",
       "  AIMessage(content=[], additional_kwargs={'reasoning': {'id': 'rs_070777c1fb554a660068ebd9a474f48192877e2a385f3499ac', 'summary': [], 'type': 'reasoning'}, '__openai_function_call_ids__': {'call_4vvwfGS9K0zszbZAmj2kbALS': 'fc_070777c1fb554a660068ebd9a61a6c819295d8cd7bb11c7325'}}, response_metadata={'id': 'resp_070777c1fb554a660068ebd9a3b0708192b1042736f30d441d', 'created_at': 1760287139.0, 'metadata': {}, 'model': 'gpt-5-nano-2025-08-07', 'object': 'response', 'service_tier': 'default', 'status': 'completed', 'model_name': 'gpt-5-nano-2025-08-07'}, id='lc_run--7de8e40a-068d-4783-a164-0f1f325277e6-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_4vvwfGS9K0zszbZAmj2kbALS', 'type': 'tool_call'}], usage_metadata={'input_tokens': 58, 'output_tokens': 277, 'total_tokens': 335, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 256}}),\n",
       "  ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', id='6983797c-d4b7-444e-9cd7-9a6114c505c9', tool_call_id='call_4vvwfGS9K0zszbZAmj2kbALS'),\n",
       "  AIMessage(content=[{'type': 'text', 'text': \"San Francisco (SF) weather: It's always sunny in San Francisco!\\n\\nWant me to pull real-time details like temperature, humidity, wind, or a short-term forecast for today or the week? If you meant a different SF (e.g., another city with the same initials), tell me and I’ll check that.\", 'annotations': []}], additional_kwargs={'reasoning': {'id': 'rs_070777c1fb554a660068ebd9a888688192869e106561bf17e2', 'summary': [], 'type': 'reasoning'}}, response_metadata={'id': 'resp_070777c1fb554a660068ebd9a6ea1481929565d4ea3912849e', 'created_at': 1760287143.0, 'metadata': {}, 'model': 'gpt-5-nano-2025-08-07', 'object': 'response', 'service_tier': 'default', 'status': 'completed', 'model_name': 'gpt-5-nano-2025-08-07'}, id='msg_070777c1fb554a660068ebd9aa9ac881928e346d15f36cf3e3', usage_metadata={'input_tokens': 386, 'output_tokens': 454, 'total_tokens': 840, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 384}})]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain.tools import tool    \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "langsmith_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "langsmith_project = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "langsmith_endpoint = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    " \n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-5-nano-2025-08-07\",\n",
    "    temperature=0.7,\n",
    "    stream_usage=True, \n",
    "    use_responses_api=True,\n",
    "    \n",
    "    \n",
    "\n",
    "     \n",
    ")\n",
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model_with_tools,\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2efa3",
   "metadata": {},
   "source": [
    "### Dynamic Model\n",
    "Dynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization.\n",
    "To use a dynamic model, create middleware with the ```@wrap_model_call``` decorator that modifies the model in the request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee1e091e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='02b25661-d659-4c49-bc88-a7bebb6fe07f'),\n",
      "              AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_HCHt71165yDeFlP9EpvabjNB', 'function': {'arguments': '{\"city\":\"San Francisco\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 131, 'total_tokens': 219, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CPe8v4VxFxBwoAStzSseppXvoQzoV', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--8a32bbbf-327a-4dce-b99a-e80bee6afb42-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_HCHt71165yDeFlP9EpvabjNB', 'type': 'tool_call'}], usage_metadata={'input_tokens': 131, 'output_tokens': 88, 'total_tokens': 219, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}}),\n",
      "              ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', id='966ac20b-89f9-4c67-9f21-9a065f40235f', tool_call_id='call_HCHt71165yDeFlP9EpvabjNB'),\n",
      "              AIMessage(content=\"San Francisco: It's always sunny in San Francisco!\\n\\nIf you’d like real-time details (temperature, humidity, wind, etc.), I can fetch current conditions for you. Want me to check now?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 497, 'prompt_tokens': 167, 'total_tokens': 664, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 448, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CPe8xCR8ZgOBh3GgBSPlXNSuot5L7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--7806fffa-0785-4130-b5ef-3efda1c1c1b3-0', usage_metadata={'input_tokens': 167, 'output_tokens': 497, 'total_tokens': 664, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 448}})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest\n",
    "from langchain.agents.middleware.types import ModelResponse\n",
    "from langchain.tools import tool\n",
    "\n",
    "basic_model = ChatOpenAI(model=\"gpt-5-nano-2025-08-07\")\n",
    "advanced_model = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "\n",
    "\n",
    "@wrap_model_call#type of middleware that takes request and handler as arguments\n",
    "@traceable\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"Choose model based on conversation complexity.\"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "\n",
    "    if message_count > 10:\n",
    "        # Use advanced model for longer conversations\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        model = basic_model\n",
    "\n",
    "    request.model = model\n",
    "    return handler(request)\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model,  # Default model\n",
    "    tools=[get_weather],\n",
    "    middleware=[dynamic_model_selection] \n",
    ")\n",
    "\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]})\n",
    "pprint(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a607f71",
   "metadata": {},
   "source": [
    "### System Prompt (both regular and dynamic)\n",
    "When no system_prompt is provided, the agent will infer its task from the messages directly.\n",
    "For more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware.\n",
    "The ```@dynamic_prompt``` decorator creates middleware that generates system prompts dynamically based on the model request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfe0409",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOllama\nmodel\n  Field required [type=missing, input_value={'model_name': 'ollama:gpt-oss:20b'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[147]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m langsmith_project = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mLANGCHAIN_PROJECT\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m langsmith_endpoint = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mLANGCHAIN_ENDPOINT\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m model = \u001b[43mChatOllama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mollama:gpt-oss:20b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m tools = {\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mweb_search_preview\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     20\u001b[39m model_with_tools = model.bind_tools([tools]) \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/acc-llamaindex/.venv/lib/python3.11/site-packages/langchain_core/load/serializable.py:113\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    112\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/acc-llamaindex/.venv/lib/python3.11/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for ChatOllama\nmodel\n  Field required [type=missing, input_value={'model_name': 'ollama:gpt-oss:20b'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    " \n",
    "from typing import TypedDict\n",
    "from pprint import pprint\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware.types import dynamic_prompt, ModelRequest, wrap_tool_call\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "langsmith_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "langsmith_project = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "langsmith_endpoint = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "\n",
    "model = ChatOllama(model_name=\"gpt-oss:20b\")\n",
    "tools = {\"type\": \"web_search_preview\"}\n",
    "model_with_tools = model.bind_tools([tools]) \n",
    "\n",
    "\n",
    "class Context(TypedDict):\n",
    "    user_role: str\n",
    "    \n",
    "    \n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\"\"\"\n",
    "    return f\"Found information about {query}\"\n",
    "\n",
    "@dynamic_prompt # convenience decorator that creates middleware using wrap_model_call specifically for dynamic prompt generation\n",
    "def user_role_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"Generate system prompt based on user role.\"\"\"\n",
    "    user_role = request.runtime.context.get(\"user_role\", \"user\")\n",
    "    base_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "    if user_role == \"expert\":\n",
    "        return f\"{base_prompt} Provide detailed technical responses.\"\n",
    "    elif user_role == \"beginner\":\n",
    "        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n",
    "\n",
    "    return base_prompt\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        # Return a custom error message to the model\n",
    "        return ToolMessage(\n",
    "            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model_with_tools,\n",
    "    # tools=[web_search],\n",
    "    context_schema=Context,\n",
    "    middleware=[handle_tool_errors, user_role_prompt]\n",
    ")\n",
    "\n",
    "# The system prompt will be set dynamically based on context\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"search the web and Explain agentic RAG based on the latest 2025 research.\"}]},\n",
    "    context={\"user_role\": \"beginner\"}, \n",
    ")\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83f622",
   "metadata": {},
   "source": [
    "### Structured Output\n",
    "LangChain provides strategies for structured output via the response_format parameter.\n",
    "\n",
    "**ToolStrategy**\n",
    "uses artificial tool calling to generate structured output. This works with any model that supports tool calling\n",
    "\n",
    "**ProviderStrategy**\n",
    "uses the model provider’s native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b45f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Extract contact info from: John Doe, john@example.com, (555) 123-4567', additional_kwargs={}, response_metadata={}, id='6d0e2575-1f63-46b0-b5c0-1050f262e25d'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_7Zmbves1v9tTZknBDuUZKpAu', 'function': {'arguments': '{\"name\":\"John Doe\",\"email\":\"john@example.com\",\"phone\":\"(555) 123-4567\"}', 'name': 'ContactInfo'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 297, 'prompt_tokens': 163, 'total_tokens': 460, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CPcZ2qnmTQys3rIVg3iDuP0cyXyVl', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--88751229-3af6-4578-8585-83bca7695904-0', tool_calls=[{'name': 'ContactInfo', 'args': {'name': 'John Doe', 'email': 'john@example.com', 'phone': '(555) 123-4567'}, 'id': 'call_7Zmbves1v9tTZknBDuUZKpAu', 'type': 'tool_call'}], usage_metadata={'input_tokens': 163, 'output_tokens': 297, 'total_tokens': 460, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}}),\n",
       "  ToolMessage(content=\"Returning structured response: name='John Doe' email='john@example.com' phone='(555) 123-4567'\", name='ContactInfo', id='0bdfb366-48fa-40b9-beb7-2c3f5699883a', tool_call_id='call_7Zmbves1v9tTZknBDuUZKpAu')],\n",
       " 'structured_response': ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ToolStrategy\n",
    "from pydantic import BaseModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    name: str\n",
    "    email: str\n",
    "    phone: str\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[search],\n",
    "    response_format=ToolStrategy(ContactInfo)\n",
    ")\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n",
    "})\n",
    "\n",
    "result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a59b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Extract contact info from: John Doe, john@example.com, (555) 123-4567', additional_kwargs={}, response_metadata={}, id='9a0adfe7-b25f-4745-b117-dd7525dccbb3'),\n",
       "  AIMessage(content='{\"name\":\"John Doe\",\"email\":\"john@example.com\",\"phone\":\"(555) 123-4567\"}', additional_kwargs={'parsed': None, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 292, 'prompt_tokens': 176, 'total_tokens': 468, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CPcYjFCGVc2iGad9KX7lRcGwN9Rpa', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--def6b71d-d37e-447b-a3b8-b39f94af9e23-0', usage_metadata={'input_tokens': 176, 'output_tokens': 292, 'total_tokens': 468, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})],\n",
       " 'structured_response': ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ProviderStrategy\n",
    "from langchain.agents.structured_output import ProviderStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    response_format=ProviderStrategy(ContactInfo)\n",
    ")\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n",
    "})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd46ad56",
   "metadata": {},
   "source": [
    "### Model methods: (suedo code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4d96b7",
   "metadata": {},
   "source": [
    "#### Invoke\n",
    "The most straightforward way to call a model is to use invoke() with a single message or a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0068f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single message\n",
    "response = model.invoke(\"Why do parrots have colorful feathers?\")\n",
    "print(response)\n",
    "\n",
    "\n",
    "# Dictionary format\n",
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to French.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Translate: I love programming.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"J'adore la programmation.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Translate: I love building applications.\"}\n",
    "]\n",
    "\n",
    "response = model.invoke(conversation)\n",
    "print(response)  # AIMessage(\"J'adore créer des applications.\")\n",
    "\n",
    "\n",
    "# message objects\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "conversation = [\n",
    "    SystemMessage(\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(\"Translate: I love programming.\"),\n",
    "    AIMessage(\"J'adore la programmation.\"),\n",
    "    HumanMessage(\"Translate: I love building applications.\")\n",
    "]\n",
    "\n",
    "response = model.invoke(conversation)\n",
    "print(response)  // AIMessage(\"J'adore créer des applications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da66b246",
   "metadata": {},
   "source": [
    "#### Stream\n",
    "Calling ```stream()``` returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time\n",
    "\n",
    "As opposed to ```invoke()```, which returns a single AIMessage after the model has finished generating its full response, ```stream()``` returns multiple AIMessageChunk objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3961e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic text streaming\n",
    "for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n",
    "    print(chunk.text, end=\"|\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "# stream toolcalls, reasoning, and othe content\n",
    "for chunk in model.stream(\"What color is the sky?\"):\n",
    "    for block in chunk.content_blocks:\n",
    "        if block[\"type\"] == \"reasoning\" and (reasoning := block.get(\"reasoning\")):\n",
    "            print(f\"Reasoning: {reasoning}\")\n",
    "        elif block[\"type\"] == \"tool_call_chunk\":\n",
    "            print(f\"Tool call chunk: {block}\")\n",
    "        elif block[\"type\"] == \"text\":\n",
    "            print(block[\"text\"])\n",
    "        else:\n",
    "            ...\n",
    "            \n",
    "            \n",
    "            \n",
    "# construct an AI message\n",
    "full = None  # None | AIMessageChunk\n",
    "for chunk in model.stream(\"What color is the sky?\"):\n",
    "    full = chunk if full is None else full + chunk\n",
    "    print(full.text)\n",
    "\n",
    "# The\n",
    "# The sky\n",
    "# The sky is\n",
    "# The sky is typically\n",
    "# The sky is typically blue\n",
    "# ...\n",
    "\n",
    "print(full.content_blocks)\n",
    "# [{\"type\": \"text\", \"text\": \"The sky is typically blue...\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc181f",
   "metadata": {},
   "source": [
    "#### Batch\n",
    "By default, ```batch()``` will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with ```batch_as_completed()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#straight batch\n",
    "responses = model.batch([\n",
    "    \"Why do parrots have colorful feathers?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"What is quantum computing?\"\n",
    "])\n",
    "for response in responses:\n",
    "    print(response)\n",
    "    \n",
    "    \n",
    "#yeid batch responses upon completion\n",
    "for response in model.batch_as_completed([\n",
    "    \"Why do parrots have colorful feathers?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"What is quantum computing?\"\n",
    "]):\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef1128",
   "metadata": {},
   "source": [
    "#### Tool Calling\n",
    "To make tools that you have defined available for use by a model, you must bind them using bind_tools(). In subsequent invocations, the model can choose to call any of the bound tools as needed.\n",
    "\n",
    "When binding user-defined tools, the model’s response includes a request to execute a tool. When using a model separately from an agent, it is up to you to perform the requested action and return the result back to the model for use in subsequent reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c51552fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "I can’t literally draw a picture here, but here’s a little ASCII‑art “cute dog” that you can copy‑paste and share:\n",
      "\n",
      "```\n",
      "   __\n",
      "o-''))_____\\/\\\n",
      " \\_/      /  )\n",
      "     ( /   /\n",
      "       `--'\n",
      "```\n",
      "\n",
      "It’s a simple, cartoonish pup with floppy ears and a wagging tail—hope it brings a smile!\n"
     ]
    }
   ],
   "source": [
    "#Binding user tools\n",
    "from langchain.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "import base64\n",
    "from IPython.display import Image\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest\n",
    "from langchain.agents.middleware.types import ModelResponse\n",
    "from langchain.messages import AIMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"gpt-oss:20b\")\n",
    "tools = [{\"type\": \"web_search_preview\", }, {\"type\": \"image_generation\", \"quality\": \"low\"}]\n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the weather at a location.\"\"\"\n",
    "    return f\"It's sunny in {location}.\"\n",
    "\n",
    "\n",
    "response = model_with_tools.invoke(\"Draw a picture of a cute dog\")\n",
    "for tool_call in response.tool_calls:\n",
    "    # View tool calls made by the model\n",
    "    print(f\"Tool: {tool_call['name']}\")\n",
    "    print(f\"Args: {tool_call['args']}\")\n",
    "\n",
    "agent = create_agent(\n",
    "    model = model_with_tools,\n",
    "    tools = tools, \n",
    ") \n",
    "result = agent.invoke({\"messages\":[\n",
    "    {\"role\": \"user\", \"content\": \"what the weather in New York?\"}\n",
    "    ]})\n",
    "\n",
    "    \n",
    "print(type(response))\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819655c3",
   "metadata": {},
   "source": [
    "Each ToolMessage returned by the tool includes a tool_call_id that matches the original tool call, helping the model correlate results with requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6d89990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s sunny in Boston.\n",
      "[{'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'c0ef9fae-e60a-4e99-9447-c1143a392468', 'type': 'tool_call'}]\n",
      "content=\"It's sunny in Boston.\" name='get_weather' tool_call_id='c0ef9fae-e60a-4e99-9447-c1143a392468'\n"
     ]
    }
   ],
   "source": [
    "#Tool execution loop - steps 1 and 2 are run under the hood by LangChain.\n",
    "\n",
    "\n",
    "# Bind (potentially multiple) tools to the model\n",
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "# Step 1: Model generates tool calls\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]\n",
    "ai_msg = model_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "\n",
    "# Step 2: Execute tools and collect results\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    # Execute the tool with the generated arguments\n",
    "    tool_result = get_weather.invoke(tool_call)\n",
    "    messages.append(tool_result)\n",
    "\n",
    "# Step 3: Pass results back to model for final response\n",
    "final_response = model_with_tools.invoke(messages)\n",
    "print(final_response.text)\n",
    "print(ai_msg.tool_calls)\n",
    "print(tool_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbb63ce",
   "metadata": {},
   "source": [
    "#### Tools (continued)\n",
    "The simplest way to create a tool is with the ```@tool``` decorator. By default, the function’s docstring becomes the tool’s description that helps the model understand when to use it\n",
    "\n",
    "Type hints are **required** as they define the tool’s input schema. The docstring should be informative and concise to help the model understand the tool’s purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic definition \n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_database(query: str, limit: int = 10) -> str:\n",
    "    \"\"\"Search the customer database for records matching the query.\n",
    "    Args:\n",
    "        query: Search terms to look for\n",
    "        limit: Maximum number of results to return\n",
    "    \"\"\"\n",
    "    return f\"Found {limit} results for '{query}'\"\n",
    "\n",
    "#Custom tool name (by default it is the function name)\n",
    "@tool(\"web_search\")  # Custom name\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\"\"\"\n",
    "    return f\"Results for: {query}\"\n",
    "\n",
    "print(search.name)  # web_search\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e641161a",
   "metadata": {},
   "source": [
    "Advanced shcema definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b15c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define complex inputs with Pydantic models \n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    " \n",
    "class WeatherInput(BaseModel):\n",
    "    \"\"\"Input for weather queries.\"\"\"\n",
    "    location: str = Field(description=\"City name or coordinates\")\n",
    "    units: Literal[\"celsius\", \"fahrenheit\"] = Field(\n",
    "        default=\"celsius\",\n",
    "        description=\"Temperature unit preference\"\n",
    "    )\n",
    "    include_forecast: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Include 5-day forecast\"\n",
    "    )\n",
    "\n",
    "@tool(args_schema=WeatherInput)\n",
    "def get_weather(location: str, units: str = \"celsius\", include_forecast: bool = False) -> str:\n",
    "    \"\"\"Get current weather and optional forecast.\"\"\"\n",
    "    temp = 22 if units == \"celsius\" else 72\n",
    "    result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\"\n",
    "    if include_forecast:\n",
    "        result += \"\\nNext 5 days: Sunny\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a19c044",
   "metadata": {},
   "source": [
    "### Messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d67e9b",
   "metadata": {},
   "source": [
    "#### Trim messages\n",
    " If you’re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last maxTokens) to use for handling the boundary.\n",
    "\n",
    "To trim message history in an agent, use ```@[pre_model_hook][create_agent]``` with the ```trim_messages``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5105ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='88220357-4dc4-4cdf-9be2-b033541a83c2'),\n",
      "              AIMessage(content='Hello, Bob! 👋 How can I help you today?', additional_kwargs={}, response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-10-15T00:04:36.349819Z', 'done': True, 'done_reason': 'stop', 'total_duration': 24816384167, 'load_duration': 5930117084, 'prompt_eval_count': 73, 'prompt_eval_duration': 17220677333, 'eval_count': 88, 'eval_duration': 1603056578, 'model_name': 'gpt-oss:20b'}, id='lc_run--9cd0cae2-da9d-416a-9057-232389055bdc-0', usage_metadata={'input_tokens': 73, 'output_tokens': 88, 'total_tokens': 161}),\n",
      "              HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='9e942a3d-c3a4-48b5-b13b-01dc088630fe'),\n",
      "              AIMessage(content='Whiskers twitch at moonlit purrs,  \\nA silent grace upon the night.  \\nEyes like amber, soft as fur,  \\nThey rule the quiet with delight.  \\n\\nIn sunlit naps, they stretch their tails,  \\nA gentle breeze of whiskered dreams;  \\nThe world around them softly pales,  \\nFor every cat’s a living gleam.', additional_kwargs={}, response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-10-15T00:04:38.866342Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2494153834, 'load_duration': 135879875, 'prompt_eval_count': 102, 'prompt_eval_duration': 181713833, 'eval_count': 116, 'eval_duration': 2122764213, 'model_name': 'gpt-oss:20b'}, id='lc_run--f4dfe507-9b33-410e-b5ad-8184dfa15cd9-0', usage_metadata={'input_tokens': 102, 'output_tokens': 116, 'total_tokens': 218}),\n",
      "              HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='c8a795c2-8b57-47ca-b870-139974678170'),\n",
      "              AIMessage(content='Bones and breezes, wagging tales,  \\nA joyous heart that never stalls.  \\nEyes that gleam with boundless gales,  \\nIn every bark, a trust that calls.  \\n\\nSunlit trails and open fields,  \\nTheir loyal paws leave traces bright;  \\nWith every step, the world feels healed,  \\nA dog’s love, pure, simple, and light.', additional_kwargs={}, response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-10-15T00:04:41.193541Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2323254750, 'load_duration': 95812166, 'prompt_eval_count': 193, 'prompt_eval_duration': 195833083, 'eval_count': 109, 'eval_duration': 1995389466, 'model_name': 'gpt-oss:20b'}, id='lc_run--4440d061-3275-42a4-b42b-d3c6e1aca7b4-0', usage_metadata={'input_tokens': 193, 'output_tokens': 109, 'total_tokens': 302}),\n",
      "              HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='cfd69662-3689-42dc-ba46-d6da3a2c9238'),\n",
      "              AIMessage(content='Your name is Bob.', additional_kwargs={}, response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-10-15T00:04:42.211786Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1011781584, 'load_duration': 94068959, 'prompt_eval_count': 285, 'prompt_eval_duration': 196226417, 'eval_count': 38, 'eval_duration': 689608957, 'model_name': 'gpt-oss:20b'}, id='lc_run--84ae5e2c-30e6-4506-abd9-74393bfbc78d-0', usage_metadata={'input_tokens': 285, 'output_tokens': 38, 'total_tokens': 323})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.middleware.types import before_model\n",
    "from langchain_core.messages.utils import trim_messages, count_tokens_approximately\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pprint\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "model = ChatOllama(model=\"gpt-oss:20b\")\n",
    "\n",
    "@before_model\n",
    "def pre_model_hook(state, runtime) -> dict[str, list[BaseMessage]]: #before_model expects runtime even though it's not used\n",
    "    \"\"\"\n",
    "    This function will be called prior to every llm call to prepare the messages for the llm.\n",
    "    \"\"\"\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=384,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    return {\"llm_input_messages\": trimmed_messages}\n",
    "\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[pre_model_hook],\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "pprint.pprint(final_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553064aa",
   "metadata": {},
   "source": [
    "### Summarization (Built-in middleware)\n",
    "Automatically summarize conversation history when approaching token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697e49b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is the square root of 16?', additional_kwargs={}, response_metadata={}, id='c0ce41d5-cab5-4af8-8704-fb18a29ed589'), AIMessage(content='The square root of 16 is \\\\(4\\\\).', additional_kwargs={}, response_metadata={'model': 'gpt-oss:20b', 'created_at': '2025-10-14T23:58:13.334686Z', 'done': True, 'done_reason': 'stop', 'total_duration': 63945702541, 'load_duration': 6948658208, 'prompt_eval_count': 155, 'prompt_eval_duration': 56280518417, 'eval_count': 35, 'eval_duration': 635508251, 'model_name': 'gpt-oss:20b'}, id='lc_run--7801a090-e719-44a3-91a5-666654eb8f8a-0', usage_metadata={'input_tokens': 155, 'output_tokens': 35, 'total_tokens': 190})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "model = ChatOllama(model=\"gpt-oss:20b\")\n",
    "\n",
    "@tool\n",
    "def weather_tool(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the weather in the given location.  \n",
    "    \"\"\"\n",
    "    return f\"The weather in {location} is sunny.\"\n",
    "\n",
    "@tool\n",
    "def calculator_tool(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the result of the given expression.\n",
    "    \"\"\"\n",
    "    return eval(expression)\n",
    "\n",
    "agent = create_agent( \n",
    "    model=model,\n",
    "    tools=[weather_tool, calculator_tool],\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=model,\n",
    "            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n",
    "            messages_to_keep=20,  # Keep last 20 messages after summary\n",
    "            summary_prompt=\"Custom prompt for summarization...\",  # Optional\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is the square root of 16?\"}]})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6806ec",
   "metadata": {},
   "source": [
    "### Human-in-the-loop (built-in middleware)\n",
    "Pause agent execution for human approval, editing, or rejection of tool calls before they execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ddc68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.agents.middleware.prompt_caching import AnthropicPromptCachingMiddleware\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "LONG_PROMPT = \"\"\"\n",
    "Please be a helpful assistant.\n",
    "\n",
    "<Lots more context ...>\n",
    "\"\"\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=ChatAnthropic(model=\"claude-sonnet-4-latest\"),\n",
    "    system_prompt=LONG_PROMPT,\n",
    "    middleware=[AnthropicPromptCachingMiddleware(ttl=\"5m\")],\n",
    ")\n",
    "\n",
    "# cache store\n",
    "agent.invoke({\"messages\": [HumanMessage(\"Hi, my name is Bob\")]})\n",
    "\n",
    "# cache hit, system prompt is cached\n",
    "agent.invoke({\"messages\": [HumanMessage(\"What's my name?\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be07fc7",
   "metadata": {},
   "source": [
    "### Tool call limit (Built-in middleware)\n",
    "Limit the number of tool calls to specific tools or all tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db64d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import ToolCallLimitMiddleware\n",
    "\n",
    "# Limit all tool calls\n",
    "global_limiter = ToolCallLimitMiddleware(thread_limit=20, run_limit=10)\n",
    "\n",
    "# Limit specific tool\n",
    "search_limiter = ToolCallLimitMiddleware(\n",
    "    tool_name=\"search\",\n",
    "    thread_limit=5,\n",
    "    run_limit=3,\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o\",\n",
    "    tools=[...],\n",
    "    middleware=[global_limiter, search_limiter],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be07d1",
   "metadata": {},
   "source": [
    "### Model Fallback\n",
    "Automatically fallback to alternative models when the primary model fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c9a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import ModelFallbackMiddleware\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o\",  # Primary model\n",
    "    tools=[...],\n",
    "    middleware=[\n",
    "        ModelFallbackMiddleware(\n",
    "            \"openai:gpt-4o-mini\",  # Try first on error\n",
    "            \"anthropic:claude-3-5-sonnet-20241022\",  # Then this\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bcc133",
   "metadata": {},
   "source": [
    "### Decorator-based middleware\n",
    "For simple middleware that only needs a single hook, decorators provide the quickest way to add functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61d068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import before_model, after_model, wrap_model_call\n",
    "from langchain.agents.middleware import AgentState, ModelRequest, ModelResponse\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import Any, Callable\n",
    "\n",
    "# Node-style: logging before model calls\n",
    "@before_model\n",
    "def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    print(f\"About to call model with {len(state['messages'])} messages\")\n",
    "    return None\n",
    "\n",
    "# Node-style: validation after model calls\n",
    "@after_model(can_jump_to=[\"end\"])\n",
    "def validate_output(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"BLOCKED\" in last_message.content:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n",
    "            \"jump_to\": \"end\"\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# Wrap-style: retry logic\n",
    "@wrap_model_call\n",
    "def retry_model(\n",
    "    request: ModelRequest,\n",
    "    handler: Callable[[ModelRequest], ModelResponse],\n",
    ") -> ModelResponse:\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            return handler(request)\n",
    "        except Exception as e:\n",
    "            if attempt == 2:\n",
    "                raise\n",
    "            print(f\"Retry {attempt + 1}/3 after error: {e}\")\n",
    "\n",
    "# Wrap-style: dynamic prompts\n",
    "@dynamic_prompt\n",
    "def personalized_prompt(request: ModelRequest) -> str:\n",
    "    user_id = request.runtime.context.get(\"user_id\", \"guest\")\n",
    "    return f\"You are a helpful assistant for user {user_id}. Be concise and friendly.\"\n",
    "\n",
    "# Use decorators in agent\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o\",\n",
    "    middleware=[log_before_model, validate_output, retry_model, personalized_prompt],\n",
    "    tools=[...],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
