# API Keys (Required)
OPENAI_API_KEY=

# Optional API Keys
ANTHROPIC_API_KEY=
HUGGINGFACE_API_KEY=
GROQ_API_KEY=
LLAMA_CLOUD_API_KEY=

# LangSmith Configuration (Optional)
LANGSMITH_API_KEY=
LANGSMITH_TRACING=true
LANGSMITH_WORKSPACE_ID=
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_PROJECT=

# Provider Selection (Optional - defaults shown)
LLM_PROVIDER=openai        # Options: openai, anthropic, groq
EMBEDDING_PROVIDER=openai  # Currently only openai supported

# Model Selection (Optional - override defaults)
# OPENAI_MODEL=gpt-4o
# ANTHROPIC_MODEL=claude-sonnet-4-20250514
# GROQ_MODEL=llama-3.3-70b-versatile

# Local Development Paths (Required for local development)
# Use absolute paths for your local machine
# Example: DOCUMENTS_PATH="/Users/yourname/coding/acc-llamaindex/data/documents"
DOCUMENTS_PATH="/path/to/your/project/data/documents"
CHROMA_DB_PATH="/path/to/your/project/data/chroma_db"

# Ingestion Configuration (Optional)
# INGESTION_BATCH_SIZE=1000

# Mem0 Configuration (Optional - for Mem0 memory system)
MEM0_ENABLED=false                           # Set to true to enable Mem0
MEM0_LLM_PROVIDER=openai                     # LLM for memory operations
MEM0_LLM_MODEL=gpt-4o-mini                   # Model for summarization/extraction
MEM0_EMBEDDER_MODEL=text-embedding-3-small   # Embedding model for memories
MEM0_ENABLE_DEDUP=true                       # Enable automatic deduplication
MEM0_HISTORY_LIMIT=10                        # Conversation history window

# Note: In Docker, these paths default to /app/data/* and don't need to be set