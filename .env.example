# ============================================================================
# COMPLIANCE INTELLIGENCE PLATFORM - ENVIRONMENT CONFIGURATION
# ============================================================================
# This file documents all environment variables used by the platform.
# Copy this file to .env and fill in your actual values.
# Never commit .env files with real credentials to version control.
# ============================================================================

# ============================================================================
# REQUIRED: LLM API KEYS
# ============================================================================
# At least one LLM provider is required for the platform to function.
# OpenAI is the primary provider and required for embeddings.

# OpenAI API Key (REQUIRED)
# Used for: Chat completions, embeddings, memory operations
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# ============================================================================
# OPTIONAL: ALTERNATIVE LLM PROVIDERS
# ============================================================================
# These providers can be used as alternatives to OpenAI for chat completions.
# Configure via LLM_PROVIDER setting below.

# Anthropic API Key (Optional)
# Used for: Claude models as alternative to OpenAI
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# Groq API Key (Optional)
# Used for: High-speed inference with open-source models
# Get your key at: https://console.groq.com/
GROQ_API_KEY=

# HuggingFace API Key (Optional)
# Used for: Cross-encoder reranking models
# Get your key at: https://huggingface.co/settings/tokens
HUGGINGFACE_API_KEY=

# Llama Cloud API Key (Optional)
# Used for: Llama model access via cloud API
# Get your key at: https://cloud.llamaindex.ai/
LLAMA_CLOUD_API_KEY=

# ============================================================================
# REQUIRED: SUPABASE DATABASE CONFIGURATION
# ============================================================================
# Supabase is the primary long-term storage for compliance data, pulse digests,
# and analytics. All three keys are required for full functionality.

# Supabase Project URL (REQUIRED)
# Format: https://your-project-id.supabase.co
# Find at: Supabase Dashboard > Project Settings > API
SUPABASE_URL=

# Supabase Anonymous Key (REQUIRED)
# Used for: Client-side operations with Row Level Security
# Find at: Supabase Dashboard > Project Settings > API > anon/public key
SUPABASE_ANON_KEY=

# Supabase Service Role Key (REQUIRED)
# Used for: Backend operations that bypass Row Level Security
# Find at: Supabase Dashboard > Project Settings > API > service_role key
# WARNING: Keep this secret! It has full database access.
SUPABASE_SERVICE_KEY=

# ============================================================================
# REQUIRED: COMPLIANCE DATA SOURCE API KEYS
# ============================================================================
# These keys enable real-time compliance data fetching from government APIs.
# Without these, the system will use fallback mock data.

# ITA Consolidated Screening List API Key (REQUIRED for sanctions data)
# Used for: Sanctions screening and denied party list checks
# Get your key at: https://developer.trade.gov/
# Note: Free tier available with rate limits
CSL_API_KEY=

# FDA Import Refusals API Key (Optional - may not require key)
# Used for: Health and safety import refusal data
# Check: https://www.fda.gov/industry/fda-data-standards-advisory-board
# Note: Some FDA APIs are public and don't require keys
FDA_API_KEY=

# ============================================================================
# OPTIONAL: LANGSMITH OBSERVABILITY
# ============================================================================
# LangSmith provides tracing, debugging, and monitoring for LLM applications.
# Highly recommended for production deployments.

# LangSmith API Key (Optional)
# Get your key at: https://smith.langchain.com/settings
LANGSMITH_API_KEY=

# Enable LangSmith Tracing (Optional)
# Set to "true" to enable, "false" to disable
LANGSMITH_TRACING=false

# LangSmith Workspace ID (Optional)
# Find at: LangSmith Dashboard > Settings > Workspace
LANGSMITH_WORKSPACE_ID=

# LangSmith API Endpoint (Optional)
# Default: https://api.smith.langchain.com
LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# LangSmith Project Name (Optional)
# Organizes traces by project in the dashboard
LANGSMITH_PROJECT=compliance-intelligence

# ============================================================================
# OPTIONAL: LLM PROVIDER SELECTION
# ============================================================================
# Configure which LLM providers to use for different operations.

# Primary LLM Provider (Optional - default: openai)
# Options: openai, anthropic, groq
# Used for: Chat completions, compliance analysis, pulse generation
LLM_PROVIDER=openai

# Embedding Provider (Optional - default: openai)
# Options: openai (currently only supported provider)
# Used for: Document embeddings, semantic search, RAG
EMBEDDING_PROVIDER=openai

# ============================================================================
# OPTIONAL: MODEL SELECTION
# ============================================================================
# Override default models for each provider. Uncomment to customize.

# OpenAI Model (Optional - default: gpt-5-nano-2025-08-07)
# Options: gpt-5-nano-2025-08-07, gpt-4o, gpt-4o-mini, gpt-3.5-turbo
# OPENAI_MODEL=gpt-5-nano-2025-08-07

# OpenAI Embedding Model (Optional - default: text-embedding-3-small)
# Options: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Anthropic Model (Optional - default: claude-haiku-4-5-20251001)
# Options: claude-haiku-4-5-20251001, claude-sonnet-4-20250514, claude-opus-4-20250514
# ANTHROPIC_MODEL=claude-haiku-4-5-20251001

# Groq Model (Optional - default: openai/gpt-oss-120b)
# Options: openai/gpt-oss-120b, llama-3.3-70b-versatile, mixtral-8x7b-32768
# GROQ_MODEL=openai/gpt-oss-120b

# ============================================================================
# REQUIRED: LOCAL DEVELOPMENT PATHS
# ============================================================================
# These paths are required for local development. Use absolute paths.
# In Docker, these are automatically set to /app/data/* (see docker-compose.yaml)

# Documents Directory (REQUIRED for local development)
# Path to directory containing documents for ingestion
# Example: /Users/yourname/projects/exim-agent/data/documents
# Docker default: /app/data/documents
DOCUMENTS_PATH=/path/to/your/project/data/documents

# ChromaDB Persistence Directory (REQUIRED for local development)
# Path to directory where ChromaDB stores vector embeddings
# Example: /Users/yourname/projects/exim-agent/data/chroma_db
# Docker default: /app/data/chroma_db
CHROMA_DB_PATH=/path/to/your/project/data/chroma_db

# Mem0 History Database Path (Optional - default: /app/data/mem0_history.db)
# Path to SQLite database for conversation history
# Example: /Users/yourname/projects/exim-agent/data/mem0_history.db
# Docker default: /app/data/mem0_history.db
# MEM0_HISTORY_DB_PATH=/path/to/your/project/data/mem0_history.db

# ============================================================================
# OPTIONAL: CHROMADB CONFIGURATION
# ============================================================================
# ChromaDB is used for short-term vector storage and semantic search.

# ChromaDB Collection Name (Optional - default: documents)
# Name of the default collection for document embeddings
# CHROMA_COLLECTION_NAME=documents

# ============================================================================
# OPTIONAL: MEM0 MEMORY SYSTEM CONFIGURATION
# ============================================================================
# Mem0 provides conversational memory management for chat interactions.
# It stores memories in ChromaDB for semantic retrieval.

# Enable Mem0 (Optional - default: true)
# Set to "true" to enable conversational memory, "false" to disable
MEM0_ENABLED=true

# Mem0 Vector Store (Optional - default: chroma)
# Backend for storing memory embeddings
# Options: chroma (uses existing ChromaDB instance)
# MEM0_VECTOR_STORE=chroma

# Mem0 LLM Provider (Optional - default: openai)
# LLM used for memory summarization and extraction
# Options: openai, anthropic, groq
MEM0_LLM_PROVIDER=openai

# Mem0 LLM Model (Optional - default: gpt-5-nano-2025-08-07)
# Specific model for memory operations (should be fast and cost-effective)
MEM0_LLM_MODEL=gpt-5-nano-2025-08-07

# Mem0 Embedder Model (Optional - default: text-embedding-3-small)
# Embedding model for memory semantic search
MEM0_EMBEDDER_MODEL=text-embedding-3-small

# Mem0 Deduplication (Optional - default: true)
# Enable automatic deduplication of similar memories
MEM0_ENABLE_DEDUP=true

# Mem0 History Limit (Optional - default: 10)
# Number of conversation turns to keep in context window
MEM0_HISTORY_LIMIT=10

# ============================================================================
# OPTIONAL: DOCUMENT INGESTION CONFIGURATION
# ============================================================================
# Settings for document processing and ingestion pipeline.

# Ingestion Batch Size (Optional - default: 1000)
# Number of documents to process in a single batch
# Larger batches = faster ingestion but more memory usage
INGESTION_BATCH_SIZE=1000

# Chunk Size (Optional - default: 1024)
# Number of characters per document chunk for embeddings
# CHUNK_SIZE=1024

# Chunk Overlap (Optional - default: 200)
# Number of overlapping characters between chunks
# CHUNK_OVERLAP=200

# ============================================================================
# OPTIONAL: RAG (RETRIEVAL-AUGMENTED GENERATION) CONFIGURATION
# ============================================================================
# Settings for semantic search and context retrieval.

# Retrieval K (Optional - default: 20)
# Number of documents to retrieve before reranking
# Higher values = more context but slower performance
# RETRIEVAL_K=20

# Retrieval Score Threshold (Optional - default: 0.7)
# Minimum similarity score for retrieved documents (0.0 to 1.0)
# Higher values = more relevant but fewer results
# RETRIEVAL_SCORE_THRESHOLD=0.7

# ============================================================================
# OPTIONAL: RERANKING CONFIGURATION
# ============================================================================
# Reranking improves RAG quality by reordering retrieved documents.
# Uses cross-encoder models for more accurate relevance scoring.

# Enable Reranking (Optional - default: true)
# Set to "true" to enable, "false" to disable
# Disabling improves performance but may reduce answer quality
ENABLE_RERANKING=true

# Rerank Top K (Optional - default: 5)
# Number of documents to keep after reranking
# Should be less than RETRIEVAL_K
# RERANK_TOP_K=5

# Cross-Encoder Model (Optional - default: cross-encoder/ms-marco-MiniLM-L-6-v2)
# HuggingFace model for reranking
# Options: cross-encoder/ms-marco-MiniLM-L-6-v2, cross-encoder/ms-marco-MiniLM-L-12-v2
# CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# ============================================================================
# OPTIONAL: EVALUATION CONFIGURATION
# ============================================================================
# Automatic evaluation of RAG responses for quality monitoring.
# Measures faithfulness, relevance, and context precision.

# Enable Evaluation (Optional - default: true)
# Set to "true" to auto-evaluate responses, "false" to disable
# Disabling improves performance but removes quality metrics
ENABLE_EVALUATION=true

# Evaluation Threshold (Optional - default: 0.7)
# Minimum acceptable quality score (0.0 to 1.0)
# Responses below this threshold are flagged for review
# EVALUATION_THRESHOLD=0.7

# ============================================================================
# OPTIONAL: LLM BEHAVIOR CONFIGURATION
# ============================================================================
# Fine-tune LLM behavior across all providers.

# Temperature (Optional - default: 0.7)
# Controls randomness in responses (0.0 = deterministic, 1.0 = creative)
# LLM_TEMPERATURE=0.7

# Max Tokens (Optional - default: None/unlimited)
# Maximum number of tokens in LLM response
# MAX_TOKENS=2000

# Streaming (Optional - default: true)
# Enable streaming responses for better UX
# STREAMING=true

# ============================================================================
# OPTIONAL: TIMEOUT CONFIGURATION
# ============================================================================
# Configure timeouts for external API calls to prevent hanging requests.
# All timeouts are in seconds.

# LLM Timeout (Optional - default: 30.0)
# Maximum time to wait for LLM API responses
# Applies to OpenAI, Anthropic, and Groq providers
LLM_TIMEOUT_SECONDS=30.0

# Tool Timeout (Optional - default: 10.0)
# Maximum time to wait for compliance tool API calls
# Applies to HTS, Sanctions, Refusals, and Rulings tools
TOOL_TIMEOUT_SECONDS=10.0

# ChromaDB Timeout (Optional - default: 5.0)
# Maximum time to wait for ChromaDB operations
# Applies to vector search and document retrieval
#CHROMA_TIMEOUT_SECONDS=5.0

# ============================================================================
# OPTIONAL: API CONFIGURATION
# ============================================================================
# Settings for the FastAPI REST API server.

# CORS Origins (Optional - default: http://localhost:3000)
# Comma-separated list of allowed origins for CORS
# Example: http://localhost:3000,https://app.example.com
# CORS_ORIGINS=http://localhost:3000

# ============================================================================
# OPTIONAL: ZENML PIPELINE ORCHESTRATION
# ============================================================================
# ZenML is used for orchestrating pulse generation pipelines.
# Local mode is used by default (no configuration needed).

# ZenML Store URL (Optional - default: local)
# URL of ZenML server for remote orchestration
# Leave empty to use local SQLite store
# ZENML_STORE_URL=

# ============================================================================
# DOCKER DEPLOYMENT NOTES
# ============================================================================
# When running in Docker (via docker-compose.yaml):
# - DOCUMENTS_PATH, CHROMA_DB_PATH, and MEM0_HISTORY_DB_PATH are automatically
#   set to /app/data/* and don't need to be configured in .env
# - The ./data directory is mounted as a volume for persistence
# - All other environment variables are passed through from .env
# - Health checks are configured to monitor API availability
#
# To deploy with Docker:
# 1. Copy this file to .env and fill in required values
# 2. Run: docker-compose up -d
# 3. Check health: curl http://localhost:8000/health
# ============================================================================